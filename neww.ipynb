{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "782ebdf8-de59-46eb-b633-6376c6fc5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "907280bc-167d-4961-9b2c-2ecd86b68d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "test_df = pd.read_csv(\"test_processed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a389753-bb0b-4baa-81d2-bf7f08c15bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cols = ['f42', 'f50', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f354']\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in cols:\n",
    "    test_df[col] = le.fit_transform(test_df[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03056bd6-0e49-4252-92cd-dd995caf2f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "missing_ratio = train_df.isnull().mean()\n",
    "cols_to_drop = missing_ratio[missing_ratio > threshold].index\n",
    "train_df = train_df.drop(columns=cols_to_drop)\n",
    "test_df = test_df.drop(columns=cols_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc61d4f9-53e5-482f-be1c-32244e17bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=[ 'id1','customer_id', 'offer_id', 'y', 'id4', 'id5'])\n",
    "y = train_df['y']\n",
    "\n",
    "X_test = test_df.drop(columns=[ 'id1','customer_id', 'offer_id', 'y','id2','id3', 'id4', 'id5'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b468f67-2d8d-475f-894a-378deba120e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = X.copy()\n",
    "X_df['y'] = y\n",
    "X_df['id1'] = train_df['id1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03aa8031-3c0c-4f4d-8ab1-541292eb92d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_df, X_val_df = train_test_split(X_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "X_train = X_train_df.drop(columns=['y', 'id1'])\n",
    "y_train = X_train_df['y']\n",
    "X_val = X_val_df.drop(columns=['y', 'id1'])\n",
    "y_val = X_val_df['y']\n",
    "\n",
    "# --- Step 6: LightGBM datasets ---\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dval = lgb.Dataset(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9be0e331-8127-4692-977b-62ba340ea646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's binary_logloss: 0.0833053\tvalid's binary_logloss: 0.0858601\n",
      "[200]\ttrain's binary_logloss: 0.0747236\tvalid's binary_logloss: 0.0795778\n",
      "[300]\ttrain's binary_logloss: 0.069618\tvalid's binary_logloss: 0.0772269\n",
      "[400]\ttrain's binary_logloss: 0.0657829\tvalid's binary_logloss: 0.0760343\n",
      "[500]\ttrain's binary_logloss: 0.0623628\tvalid's binary_logloss: 0.0751655\n",
      "[600]\ttrain's binary_logloss: 0.0594391\tvalid's binary_logloss: 0.0745321\n",
      "[700]\ttrain's binary_logloss: 0.056867\tvalid's binary_logloss: 0.0739772\n",
      "[800]\ttrain's binary_logloss: 0.0543312\tvalid's binary_logloss: 0.0735528\n",
      "[900]\ttrain's binary_logloss: 0.0521492\tvalid's binary_logloss: 0.0731792\n",
      "[1000]\ttrain's binary_logloss: 0.0501754\tvalid's binary_logloss: 0.0728733\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttrain's binary_logloss: 0.0501754\tvalid's binary_logloss: 0.0728733\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# --- Step 7: Train the model ---\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    valid_sets=[dtrain, dval],\n",
    "    valid_names=['train', 'valid'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef76bf4d-144b-4298-a3d9-5127a397d0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final submission file saved as 'final_submission_ranked.csv'\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Add raw prediction scores from your LightGBM model\n",
    "test_df['raw_pred'] = probs  # probs = model.predict(X_test)\n",
    "\n",
    "# Step 2: Rank offers within each id2 group (id2 = customer/group ID)\n",
    "test_df['rank'] = test_df.groupby('id2')['raw_pred'].rank(method='first', ascending=False)\n",
    "\n",
    "# Step 3: Convert ranks into inverse decimal scores\n",
    "test_df['inv_rank'] = test_df.groupby('id2')['rank'].transform(lambda r: 1 / r)\n",
    "\n",
    "# Step 4: Normalize inverse scores so each customer's predictions sum to 1\n",
    "test_df['pred'] = test_df.groupby('id2')['inv_rank'].transform(lambda x: x / x.sum())\n",
    "\n",
    "# Step 5: Create final submission DataFrame\n",
    "submission_df = test_df[['id1', 'id2', 'id3', 'id4', 'id5', 'pred']].copy()\n",
    "submission_df.to_csv(\"final_submission_rankedqwertqwerty.csv\", index=False)\n",
    "\n",
    "print(\"✅ Final submission file saved as 'final_submission_ranked.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7188892-e1b6-4db1-8d72-1a20cdc210f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final submission file saved as 'final_submission_ranked.csv' (without id4)\n"
     ]
    }
   ],
   "source": [
    "# Step 5 (Updated): Create final submission DataFrame without 'id4'\n",
    "submission_df = test_df[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "submission_df.to_csv(\"final_submission_rankedfinalfinal.csv\", index=False)\n",
    "\n",
    "print(\"✅ Final submission file saved as 'final_submission_ranked.csv' (without id4)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e133eefc-5c5f-409b-8abb-1101258ee641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "326e1b97-1853-409b-88a0-87b2939950bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "test_df = pd.read_csv(\"test_processed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ec2969-3290-40a3-8711-bd75da93a94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ce2d57-15e2-4325-9220-19a320a183e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "missing_ratio = train_df.isnull().mean()\n",
    "cols_to_drop = missing_ratio[missing_ratio > threshold].index\n",
    "train_df = train_df.drop(columns=cols_to_drop)\n",
    "test_df = test_df.drop(columns=cols_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8128fc05-deb4-4af4-bfe2-1ad5f6d22589",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.fillna(train_df.mean(numeric_only=True))\n",
    "test_df = test_df.fillna(train_df.mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea974dcc-46d3-4898-8141-a2febb84f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=[ 'id1','customer_id', 'offer_id', 'y', 'id4', 'id5'])\n",
    "y = train_df['y']\n",
    "\n",
    "X_test = test_df.drop(columns=[ 'id1','customer_id', 'offer_id', 'y','id2','id3', 'id4', 'id5'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f54a59a-79c7-40e9-a249-936a4c82993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_temp = X.copy()\n",
    "train_temp['y'] = y\n",
    "train_temp['id1'] = train_df['id1']\n",
    "X_train_df, X_val_df = train_test_split(train_temp, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = X_train_df.drop(columns=['y', 'id1'])\n",
    "y_train = X_train_df['y']\n",
    "X_val = X_val_df.drop(columns=['y', 'id1'])\n",
    "y_val = X_val_df['y']\n",
    "\n",
    "# --- Step 6: LightGBM training ---\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dval = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    valid_sets=[dtrain, dval],\n",
    "    valid_names=['train', 'valid'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Step 7: Predict ---\n",
    "preds = model.predict(X_test)\n",
    "\n",
    "# --- Step 8: Prepare prediction dataframe ---\n",
    "test_df['raw_pred'] = preds\n",
    "\n",
    "def normalize_group(df):\n",
    "    if len(df) == 1:\n",
    "        df['pred'] = 1.0\n",
    "    else:\n",
    "        df['pred'] = softmax(df['raw_pred'].values)\n",
    "    return df\n",
    "\n",
    "submission_df = test_df.groupby('id1', group_keys=False).apply(normalize_group)\n",
    "final_submission = submission_df[['id2', 'id3', 'id4', 'id5', 'pred']]\n",
    "final_submission.to_csv(\"final_submission_ligcdcscdcdht.csv\", index=False)\n",
    "\n",
    "print(\"✅ LightGBM prediction pipeline completed and saved to 'final_submission_light.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b5c5c-e3db-49d8-8e19-ad3c3ae561ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98f0613f-3e74-4360-89bd-0bcf80f028c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = X.copy()\n",
    "X_df['y'] = y.values\n",
    "X_df['id1'] = train_df['id1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87f424e9-e353-4fec-8062-9bc4a77d48f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_df, X_val_df = train_test_split(X_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "X_train = X_train_df.drop(columns=['y', 'id1'])\n",
    "y_train = X_train_df['y']\n",
    "X_val = X_val_df.drop(columns=['y', 'id1'])\n",
    "y_val = X_val_df['y']\n",
    "\n",
    "# Create LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1468ff6-4d32-46d6-ac87-fd7b03f9d6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's binary_logloss: 0.0841222\tvalid's binary_logloss: 0.0865231\n",
      "[200]\ttrain's binary_logloss: 0.0753358\tvalid's binary_logloss: 0.0800866\n",
      "[300]\ttrain's binary_logloss: 0.0704378\tvalid's binary_logloss: 0.0778314\n",
      "[400]\ttrain's binary_logloss: 0.0664592\tvalid's binary_logloss: 0.0764335\n",
      "[500]\ttrain's binary_logloss: 0.0631347\tvalid's binary_logloss: 0.0753905\n",
      "[600]\ttrain's binary_logloss: 0.0602427\tvalid's binary_logloss: 0.0747507\n",
      "[700]\ttrain's binary_logloss: 0.0576201\tvalid's binary_logloss: 0.0742061\n",
      "[800]\ttrain's binary_logloss: 0.0552935\tvalid's binary_logloss: 0.0737356\n",
      "[900]\ttrain's binary_logloss: 0.0531529\tvalid's binary_logloss: 0.0733852\n",
      "[1000]\ttrain's binary_logloss: 0.0510209\tvalid's binary_logloss: 0.07303\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttrain's binary_logloss: 0.0510209\tvalid's binary_logloss: 0.07303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahee\\AppData\\Local\\Temp\\ipykernel_23628\\338434572.py:47: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  submission_df = pred_df.groupby('id1', group_keys=False).apply(normalize_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LightGBM pipeline complete. File saved as 'final_submission_light.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# --- Step 6: Train the LightGBM model ---\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[train_data, val_data],\n",
    "    valid_names=['train', 'valid'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Step 7: Predict on Test Set ---\n",
    "probs = model.predict(X_test)\n",
    "\n",
    "test_sample = test_df.copy()\n",
    "test_sample['id2'] = test_sample.get('id2', pd.Series([0]*len(test_sample)))\n",
    "test_sample['id3'] = test_sample.get('id3', pd.Series([0]*len(test_sample)))\n",
    "test_sample['id4'] = test_sample.get('id4', pd.Series([0]*len(test_sample)))\n",
    "test_sample['id5'] = test_sample.get('id5', pd.Series([0]*len(test_sample)))\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    'id1': test_sample['id1'].values,\n",
    "    'id2': test_sample['id2'].values,\n",
    "    'id3': test_sample['id3'].values,\n",
    "    'id4': test_sample['id4'].values,\n",
    "    'id5': test_sample['id5'].values,\n",
    "    'raw_pred': probs\n",
    "})\n",
    "\n",
    "def normalize_group(df):\n",
    "    if len(df) == 1:\n",
    "        df['pred'] = 1.0\n",
    "    else:\n",
    "        df['pred'] = softmax(df['raw_pred'].values)\n",
    "    return df\n",
    "\n",
    "submission_df = pred_df.groupby('id1', group_keys=False).apply(normalize_group)\n",
    "final_submission = submission_df[['id2', 'id3', 'id4', 'id5', 'pred']]\n",
    "final_submission.to_csv(\"final_submission_lightnewwwww.csv\", index=False)\n",
    "\n",
    "print(\"✅ LightGBM pipeline complete. File saved as 'final_submission_light.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf9bdd-13f0-4b68-b30f-d9de45e615b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c5f7b1-d051-422b-a2ef-6d0f6753370d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b54b2ee-dc8f-4d2e-b860-6ffb71c87c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d301e-35e1-46aa-84ef-cc3bd675ad21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfcb10a-1024-4113-a06a-c7ab16e48fd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9934c9-8134-4e9b-8285-b22371ea0654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ec4b26-9661-45f3-a4a3-575c8d2464c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37143c23-8271-4b6c-a4e6-fa63725670e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ca8de-bdfc-44e4-ba78-deb5987db813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb21bf0-8956-4504-bb52-722a0cb43888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a856223b-4218-4113-b6fb-992c758bf847",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X_pca)\n",
    "X_df['y'] = y.values\n",
    "X_df['id1'] = train_df['id1'].values\n",
    "\n",
    "X_train_df, X_val_df = train_test_split(X_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "X_train = X_train_df.drop(columns=['y', 'id1'])\n",
    "y_train = X_train_df['y']\n",
    "X_val = X_val_df.drop(columns=['y', 'id1'])\n",
    "y_val = X_val_df['y']\n",
    "\n",
    "# --- Step 9: LightGBM Dataset and Parameters ---\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dval = lgb.Dataset(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d25662a-c3e6-4da3-afd8-1813983f20b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76c65a71-19c2-4afd-84d3-d1c060499366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's binary_logloss: 0.104471\tvalid's binary_logloss: 0.108146\n",
      "[200]\ttrain's binary_logloss: 0.0943352\tvalid's binary_logloss: 0.101043\n",
      "[300]\ttrain's binary_logloss: 0.0879674\tvalid's binary_logloss: 0.098046\n",
      "[400]\ttrain's binary_logloss: 0.0831459\tvalid's binary_logloss: 0.0964051\n",
      "[500]\ttrain's binary_logloss: 0.0788507\tvalid's binary_logloss: 0.0951112\n",
      "[600]\ttrain's binary_logloss: 0.0750344\tvalid's binary_logloss: 0.0941072\n",
      "[700]\ttrain's binary_logloss: 0.0714621\tvalid's binary_logloss: 0.0932328\n",
      "[800]\ttrain's binary_logloss: 0.0681398\tvalid's binary_logloss: 0.0925257\n",
      "[900]\ttrain's binary_logloss: 0.0651303\tvalid's binary_logloss: 0.091931\n",
      "[1000]\ttrain's binary_logloss: 0.062334\tvalid's binary_logloss: 0.0914136\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttrain's binary_logloss: 0.062334\tvalid's binary_logloss: 0.0914136\n"
     ]
    }
   ],
   "source": [
    "model = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    valid_sets=[dtrain, dval],\n",
    "    valid_names=['train', 'valid'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74931a57-3532-465f-bd66-3e79bb3fae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = model.predict(X_test_pca)\n",
    "\n",
    "# --- Step 12: Build Submission ---\n",
    "test_sample = test_df.copy()\n",
    "\n",
    "for col in ['id2', 'id3', 'id4', 'id5']:\n",
    "    if col not in test_sample:\n",
    "        test_sample[col] = 0\n",
    "\n",
    "pred_df = pd.DataFrame({\n",
    "    'id1': test_sample['id1'].values,\n",
    "    'id2': test_sample['id2'].values,\n",
    "    'id3': test_sample['id3'].values,\n",
    "    'id4': test_sample['id4'].values,\n",
    "    'id5': test_sample['id5'].values,\n",
    "    'raw_pred': probs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e26b5afa-3b17-4649-84fc-c9660c381892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahee\\AppData\\Local\\Temp\\ipykernel_22992\\3125717129.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  submission_df = pred_df.groupby('id1', group_keys=False).apply(normalize_group)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Pipeline complete. File saved as 'final_submission_light.csv'\n"
     ]
    }
   ],
   "source": [
    "def normalize_group(df):\n",
    "    if len(df) == 1:\n",
    "        df['pred'] = 1.0\n",
    "    else:\n",
    "        df['pred'] = softmax(df['raw_pred'].values)\n",
    "    return df\n",
    "\n",
    "submission_df = pred_df.groupby('id1', group_keys=False).apply(normalize_group)\n",
    "final_submission = submission_df[['id2', 'id3', 'id4', 'id5', 'pred']]\n",
    "final_submission.to_csv(\"final_submission_light.csv\", index=False)\n",
    "\n",
    "print(\"✅ Pipeline complete. File saved as 'final_submission_light.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d348fc-e88a-42f1-a0a2-57d973e3f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cols = ['f42', 'f50', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f354']\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in cols:\n",
    "    test_df[col] = le.fit_transform(test_df[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b21321-61f8-4ce0-ad41-034b6517b0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3d0691-a8dc-40f6-8e98-8d39ac9b3e33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading datasets...\n",
      "🔧 Preprocessing features...\n",
      "🛠️  Feature engineering...\n",
      "🛠️  Feature engineering...\n",
      "📈 Training ensemble model...\n",
      "\n",
      "🔁 Fold 1/5\n",
      "[LightGBM] [Info] Total groups: 37240, total data: 616131\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.249163 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47280\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 296\n",
      "[LightGBM] [Info] Total groups: 9310, total data: 154033\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[143]\tvalid_0's map@7: 0.932494\n",
      "\n",
      "🔁 Fold 2/5\n",
      "[LightGBM] [Info] Total groups: 37240, total data: 616131\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.250768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47392\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 297\n",
      "[LightGBM] [Info] Total groups: 9310, total data: 154033\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[127]\tvalid_0's map@7: 0.930936\n",
      "\n",
      "🔁 Fold 3/5\n",
      "[LightGBM] [Info] Total groups: 37240, total data: 616131\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.219273 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47331\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 297\n",
      "[LightGBM] [Info] Total groups: 9310, total data: 154033\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[111]\tvalid_0's map@7: 0.935649\n",
      "\n",
      "🔁 Fold 4/5\n",
      "[LightGBM] [Info] Total groups: 37240, total data: 616131\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.218429 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47353\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 296\n",
      "[LightGBM] [Info] Total groups: 9310, total data: 154033\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[98]\tvalid_0's map@7: 0.936253\n",
      "\n",
      "🔁 Fold 5/5\n",
      "[LightGBM] [Info] Total groups: 37240, total data: 616132\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.216413 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47364\n",
      "[LightGBM] [Info] Number of data points in the train set: 616132, number of used features: 297\n",
      "[LightGBM] [Info] Total groups: 9310, total data: 154032\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[240]\tvalid_0's map@7: 0.937736\n",
      "🔗 Blending predictions...\n",
      "📤 Generating predictions and evaluating MAP@7...\n",
      "\n",
      "📊 Final OOF MAP@7 Score: 0.049252\n",
      "🧪 Generating submission file...\n",
      "\n",
      "✅ Submission file saved as 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRanker\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===================================================================================\n",
    "# 1. Custom MAP@7 Metric\n",
    "# ===================================================================================\n",
    "def mapk(actuals, predicteds, k=7):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actuals, predicteds)])\n",
    "\n",
    "def apk(actual, predicted, k=7):\n",
    "    if len(predicted) > k: predicted = predicted[:k]\n",
    "    score, num_hits = 0.0, 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    if not actual: return 0.0\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# ===================================================================================\n",
    "# 2. Load and Prepare Data\n",
    "# ===================================================================================\n",
    "print(\"📥 Loading datasets...\")\n",
    "train = pd.read_parquet('train_data.parquet')\n",
    "test = pd.read_parquet('test_data.parquet')\n",
    "offer_meta = pd.read_parquet('offer_metadata.parquet')\n",
    "events = pd.read_parquet('add_event.parquet')\n",
    "trans = pd.read_parquet('add_trans.parquet')\n",
    "\n",
    "print(\"🔧 Preprocessing features...\")\n",
    "for df in [train, test]:\n",
    "    for col in df.columns:\n",
    "        if col.startswith('f'):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df['id5'] = pd.to_datetime(df['id5'], errors='coerce')\n",
    "    if 'y' in df.columns:\n",
    "        df['y'] = pd.to_numeric(df['y'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "for df in [train, test, offer_meta, events, trans]:\n",
    "    for col in ['id2', 'id3', 'customer_id', 'offer_id']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "offer_meta.rename(columns={'id3': 'offer_id', 'id9': 'industry_code', 'id12': 'start_date', 'id13': 'end_date'}, inplace=True)\n",
    "events.rename(columns={'id2': 'customer_id', 'id3': 'offer_id', 'id4': 'impression_timestamp', 'id7': 'click_timestamp'}, inplace=True)\n",
    "trans.rename(columns={'id2': 'customer_id', 'f367': 'transaction_amount', 'f370': 'transaction_date_d', 'f371': 'transaction_date_t'}, inplace=True)\n",
    "trans['transaction_date'] = pd.to_datetime(trans['transaction_date_d'] + ' ' + trans['transaction_date_t'], errors='coerce')\n",
    "\n",
    "# ===================================================================================\n",
    "# 3. Feature Engineering\n",
    "# ===================================================================================\n",
    "def feature_engineer(df):\n",
    "    print(\"🛠️  Feature engineering...\")\n",
    "    df = df.merge(offer_meta, left_on='id3', right_on='offer_id', how='left')\n",
    "    df['imp_date'] = pd.to_datetime(df['id5'])\n",
    "    df['imp_dow'] = df['imp_date'].dt.dayofweek\n",
    "    df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "    df['end_date'] = pd.to_datetime(df['end_date'], errors='coerce')\n",
    "    df['offer_duration'] = (df['end_date'] - df['start_date']).dt.days\n",
    "\n",
    "    events['click_timestamp'] = pd.to_datetime(events['click_timestamp'], errors='coerce')\n",
    "    events['impression_timestamp'] = pd.to_datetime(events['impression_timestamp'], errors='coerce')\n",
    "    events['has_clicked'] = events['click_timestamp'].notna().astype(int)\n",
    "\n",
    "    cust_agg = events.groupby('customer_id').agg(\n",
    "        total_customer_imps=('impression_timestamp', 'count'),\n",
    "        total_customer_clicks=('has_clicked', 'sum'),\n",
    "        last_click_date=('click_timestamp', lambda x: pd.to_datetime(x).max())\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "    df['days_since_last_any_click'] = (df['imp_date'] - df['last_click_date']).dt.days\n",
    "\n",
    "    cust_industry_agg = events.merge(offer_meta[['offer_id', 'industry_code']], on='offer_id')\n",
    "    cust_industry_agg = cust_industry_agg.groupby(['customer_id', 'industry_code']).agg(\n",
    "        customer_industry_imps=('impression_timestamp', 'count'),\n",
    "        customer_industry_clicks=('has_clicked', 'sum')\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_industry_agg, left_on=['id2', 'industry_code'], right_on=['customer_id', 'industry_code'], how='left')\n",
    "\n",
    "    trans['transaction_amount'] = pd.to_numeric(trans['transaction_amount'], errors='coerce')\n",
    "    trans_agg = trans.groupby('customer_id').agg(\n",
    "        avg_spend=('transaction_amount', 'mean'),\n",
    "        last_trans_date=('transaction_date', lambda x: pd.to_datetime(x).max())\n",
    "    ).reset_index()\n",
    "    df = df.merge(trans_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = feature_engineer(train)\n",
    "test = feature_engineer(test)\n",
    "\n",
    "feature_cols = [c for c in train.columns if c.startswith('f') and train[c].dtype in [np.float32, np.float64, np.int32, np.int64]] + [\n",
    "    'imp_dow', 'offer_duration', 'total_customer_imps', 'total_customer_clicks',\n",
    "    'days_since_last_any_click', 'customer_industry_imps', 'customer_industry_clicks', 'avg_spend']\n",
    "\n",
    "train.fillna(0, inplace=True)\n",
    "test.fillna(0, inplace=True)\n",
    "\n",
    "X = train[feature_cols + ['id2']]\n",
    "y = train['y']\n",
    "X_test = test[feature_cols + ['id2']]\n",
    "\n",
    "# ===================================================================================\n",
    "# 4. Ensemble Training with Hyperparameter Tuning\n",
    "# ===================================================================================\n",
    "def train_ensemble(X, y, groups, X_test):\n",
    "    print(\"📈 Training ensemble model...\")\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    oof_lgb, oof_xgb = np.zeros(len(X)), np.zeros(len(X))\n",
    "    test_lgb, test_xgb = np.zeros(len(X_test)), np.zeros(len(X_test))\n",
    "\n",
    "    for fold, (t_idx, v_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "        print(f\"\\n🔁 Fold {fold+1}/5\")\n",
    "        Xt, Xv = X.iloc[t_idx], X.iloc[v_idx]\n",
    "        yt, yv = y.iloc[t_idx], y.iloc[v_idx]\n",
    "\n",
    "        gr_t = Xt['id2'].value_counts().sort_index().values\n",
    "        gr_v = Xv['id2'].value_counts().sort_index().values\n",
    "\n",
    "        Xt_, Xv_ = Xt.drop('id2', axis=1), Xv.drop('id2', axis=1)\n",
    "        Xtest_ = X_test.drop('id2', axis=1)\n",
    "\n",
    "        model_lgb = lgb.LGBMRanker(objective='lambdarank', metric='map', eval_at=7, n_estimators=500, learning_rate=0.05, num_leaves=80)\n",
    "        model_lgb.fit(Xt_, yt, group=gr_t, eval_set=[(Xv_, yv)], eval_group=[gr_v], callbacks=[lgb.early_stopping(50)])\n",
    "        oof_lgb[v_idx] = model_lgb.predict(Xv_)\n",
    "        test_lgb += model_lgb.predict(Xtest_)\n",
    "\n",
    "        model_xgb = XGBRanker(objective='rank:pairwise', learning_rate=0.05, n_estimators=500, max_depth=6, verbosity=0)\n",
    "        model_xgb.fit(Xt_, yt, group=gr_t)\n",
    "        oof_xgb[v_idx] = model_xgb.predict(Xv_)\n",
    "        test_xgb += model_xgb.predict(Xtest_)\n",
    "\n",
    "    test_lgb /= gkf.get_n_splits()\n",
    "    test_xgb /= gkf.get_n_splits()\n",
    "\n",
    "    print(\"🔗 Blending predictions...\")\n",
    "    oof_ensemble = 0.6 * oof_lgb + 0.4 * oof_xgb\n",
    "    test_ensemble = 0.6 * test_lgb + 0.4 * test_xgb\n",
    "\n",
    "    return oof_ensemble, test_ensemble\n",
    "\n",
    "# ===================================================================================\n",
    "# 5. Inference and Submission\n",
    "# ===================================================================================\n",
    "oof, test_preds = train_ensemble(X, y, X['id2'], X_test)\n",
    "train['pred'] = oof\n",
    "\n",
    "print(\"📤 Generating predictions and evaluating MAP@7...\")\n",
    "oof_ranked = train.sort_values(['id2','pred'], ascending=[True,False]).groupby('id2')['id3'].apply(list).reset_index(name='predicted_offers')\n",
    "true_offers = train[train['y']==1].groupby('id2')['id3'].apply(list).reset_index(name='true_offers')\n",
    "eval_df = oof_ranked.merge(true_offers, on='id2', how='left')\n",
    "eval_df['true_offers'] = eval_df['true_offers'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "final_score = mapk(eval_df['true_offers'], eval_df['predicted_offers'])\n",
    "print(f\"\\n📊 Final OOF MAP@7 Score: {final_score:.6f}\")\n",
    "\n",
    "print(\"🧪 Generating submission file...\")\n",
    "scaler = MinMaxScaler()\n",
    "test['pred'] = scaler.fit_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "submission = test[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "submission['id5'] = pd.to_datetime(submission['id5']).dt.strftime('%m/%d/%Y')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\n✅ Submission file saved as 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2d7c96-82a7-4169-affb-c3ea8d295769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744431b-3919-4739-a30e-42f6d3953d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRanker\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===================================================================================\n",
    "# 1. Custom MAP@7 Metric\n",
    "# ===================================================================================\n",
    "def mapk(actuals, predicteds, k=7):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actuals, predicteds)])\n",
    "\n",
    "def apk(actual, predicted, k=7):\n",
    "    if len(predicted) > k: predicted = predicted[:k]\n",
    "    score, num_hits = 0.0, 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    if not actual: return 0.0\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# ===================================================================================\n",
    "# 2. Load and Prepare Data\n",
    "# ===================================================================================\n",
    "print(\"📥 Loading datasets...\")\n",
    "train = pd.read_parquet('train_data.parquet')\n",
    "test = pd.read_parquet('test_data.parquet')\n",
    "offer_meta = pd.read_parquet('offer_metadata.parquet')\n",
    "events = pd.read_parquet('add_event.parquet')\n",
    "trans = pd.read_parquet('add_trans.parquet')\n",
    "\n",
    "print(\"🔧 Preprocessing features...\")\n",
    "for df in [train, test]:\n",
    "    for col in df.columns:\n",
    "        if col.startswith('f'):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df['id5'] = pd.to_datetime(df['id5'], errors='coerce')\n",
    "    if 'y' in df.columns:\n",
    "        df['y'] = pd.to_numeric(df['y'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "for df in [train, test, offer_meta, events, trans]:\n",
    "    for col in ['id2', 'id3', 'customer_id', 'offer_id']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "offer_meta.rename(columns={'id3': 'offer_id', 'id9': 'industry_code', 'id12': 'start_date', 'id13': 'end_date'}, inplace=True)\n",
    "events.rename(columns={'id2': 'customer_id', 'id3': 'offer_id', 'id4': 'impression_timestamp', 'id7': 'click_timestamp'}, inplace=True)\n",
    "trans.rename(columns={'id2': 'customer_id', 'f367': 'transaction_amount', 'f370': 'transaction_date_d', 'f371': 'transaction_date_t'}, inplace=True)\n",
    "trans['transaction_date'] = pd.to_datetime(trans['transaction_date_d'] + ' ' + trans['transaction_date_t'], errors='coerce')\n",
    "\n",
    "# ===================================================================================\n",
    "# 3. Feature Engineering\n",
    "# ===================================================================================\n",
    "def feature_engineer(df):\n",
    "    print(\"🛠️  Feature engineering...\")\n",
    "    df = df.merge(offer_meta, left_on='id3', right_on='offer_id', how='left')\n",
    "    df['imp_date'] = pd.to_datetime(df['id5'])\n",
    "    df['imp_dow'] = df['imp_date'].dt.dayofweek\n",
    "    df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "    df['end_date'] = pd.to_datetime(df['end_date'], errors='coerce')\n",
    "    df['offer_duration'] = (df['end_date'] - df['start_date']).dt.days\n",
    "\n",
    "    events['click_timestamp'] = pd.to_datetime(events['click_timestamp'], errors='coerce')\n",
    "    events['impression_timestamp'] = pd.to_datetime(events['impression_timestamp'], errors='coerce')\n",
    "    events['has_clicked'] = events['click_timestamp'].notna().astype(int)\n",
    "\n",
    "    cust_agg = events.groupby('customer_id').agg(\n",
    "        total_customer_imps=('impression_timestamp', 'count'),\n",
    "        total_customer_clicks=('has_clicked', 'sum'),\n",
    "        last_click_date=('click_timestamp', lambda x: pd.to_datetime(x.dropna()).max())\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "    df['days_since_last_any_click'] = (df['imp_date'] - df['last_click_date']).dt.days\n",
    "\n",
    "    cust_industry_agg = events.merge(offer_meta[['offer_id', 'industry_code']], on='offer_id')\n",
    "    cust_industry_agg = cust_industry_agg.groupby(['customer_id', 'industry_code']).agg(\n",
    "        customer_industry_imps=('impression_timestamp', 'count'),\n",
    "        customer_industry_clicks=('has_clicked', 'sum')\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_industry_agg, left_on=['id2', 'industry_code'], right_on=['customer_id', 'industry_code'], how='left')\n",
    "\n",
    "    trans['transaction_amount'] = pd.to_numeric(trans['transaction_amount'], errors='coerce')\n",
    "    trans_agg = trans.groupby('customer_id').agg(\n",
    "        avg_spend=('transaction_amount', 'mean'),\n",
    "        last_trans_date=('transaction_date', lambda x: pd.to_datetime(x.dropna()).max())\n",
    "    ).reset_index()\n",
    "    df = df.merge(trans_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = feature_engineer(train)\n",
    "test = feature_engineer(test)\n",
    "\n",
    "feature_cols = [c for c in train.columns if c.startswith('f') and train[c].dtype in [np.float32, np.float64, np.int32, np.int64]] + [\n",
    "    'imp_dow', 'offer_duration', 'total_customer_imps', 'total_customer_clicks',\n",
    "    'days_since_last_any_click', 'customer_industry_imps', 'customer_industry_clicks', 'avg_spend']\n",
    "\n",
    "train.fillna(0, inplace=True)\n",
    "test.fillna(0, inplace=True)\n",
    "\n",
    "X = train[feature_cols + ['id2']]\n",
    "y = train['y']\n",
    "X_test = test[feature_cols + ['id2']]\n",
    "\n",
    "# ===================================================================================\n",
    "# 4. Ensemble Training with LightGBM + XGBoost\n",
    "# ===================================================================================\n",
    "def train_ensemble(X, y, groups, X_test):\n",
    "    print(\"📈 Training ensemble model...\")\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    oof_lgb, oof_xgb = np.zeros(len(X)), np.zeros(len(X))\n",
    "    test_lgb, test_xgb = np.zeros(len(X_test)), np.zeros(len(X_test))\n",
    "\n",
    "    for fold, (t_idx, v_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "        print(f\"\\n🔁 Fold {fold+1}/5\")\n",
    "        Xt, Xv = X.iloc[t_idx], X.iloc[v_idx]\n",
    "        yt, yv = y.iloc[t_idx], y.iloc[v_idx]\n",
    "\n",
    "        gr_t = Xt['id2'].value_counts().sort_index().values\n",
    "        gr_v = Xv['id2'].value_counts().sort_index().values\n",
    "\n",
    "        Xt_, Xv_ = Xt.drop('id2', axis=1), Xv.drop('id2', axis=1)\n",
    "        Xtest_ = X_test.drop('id2', axis=1)\n",
    "\n",
    "        model_lgb = lgb.LGBMRanker(objective='lambdarank', metric='map', eval_at=7, n_estimators=500, learning_rate=0.05, num_leaves=80)\n",
    "        model_lgb.fit(Xt_, yt, group=gr_t, eval_set=[(Xv_, yv)], eval_group=[gr_v], callbacks=[lgb.early_stopping(50)])\n",
    "        oof_lgb[v_idx] = model_lgb.predict(Xv_)\n",
    "        test_lgb += model_lgb.predict(Xtest_)\n",
    "\n",
    "        model_xgb = XGBRanker(objective='rank:pairwise', learning_rate=0.05, n_estimators=500, max_depth=6, verbosity=0)\n",
    "        model_xgb.fit(Xt_, yt, group=gr_t)\n",
    "        oof_xgb[v_idx] = model_xgb.predict(Xv_)\n",
    "        test_xgb += model_xgb.predict(Xtest_)\n",
    "\n",
    "    test_lgb /= gkf.get_n_splits()\n",
    "    test_xgb /= gkf.get_n_splits()\n",
    "\n",
    "    print(\"🔗 Blending predictions...\")\n",
    "    oof_ensemble = 0.6 * oof_lgb + 0.4 * oof_xgb\n",
    "    test_ensemble = 0.6 * test_lgb + 0.4 * test_xgb\n",
    "\n",
    "    return oof_ensemble, test_ensemble\n",
    "\n",
    "# ===================================================================================\n",
    "# 5. Inference and Submission\n",
    "# ===================================================================================\n",
    "oof, test_preds = train_ensemble(X, y, X['id2'], X_test)\n",
    "train['pred'] = oof\n",
    "\n",
    "print(\"📤 Generating predictions and evaluating MAP@7...\")\n",
    "oof_ranked = train.sort_values(['id2','pred'], ascending=[True,False]).groupby('id2')['id3'].apply(list).reset_index(name='predicted_offers')\n",
    "true_offers = train[train['y']==1].groupby('id2')['id3'].apply(list).reset_index(name='true_offers')\n",
    "eval_df = oof_ranked.merge(true_offers, on='id2', how='left')\n",
    "eval_df['true_offers'] = eval_df['true_offers'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "final_score = mapk(eval_df['true_offers'], eval_df['predicted_offers'])\n",
    "print(f\"\\n📊 Final OOF MAP@7 Score: {final_score:.6f}\")\n",
    "\n",
    "print(\"🧪 Generating submission file...\")\n",
    "scaler = MinMaxScaler()\n",
    "test['pred'] = scaler.fit_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "submission = test[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "submission['id5'] = pd.to_datetime(submission['id5']).dt.strftime('%m/%d/%Y')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\n✅ Submission file saved as 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c2df60-8e3c-45a4-8b0b-6d9c1a2e2a12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b8725e-f715-480b-bce2-6f868ac8fc9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b880c8f4-0131-459a-a29a-e8a1f0f25dca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9a0c4f-31dd-47c0-8d60-1ad100dee823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f517431-323a-4f31-bf69-2aab533e0129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8094e91-fff5-4da9-8454-3a678f494be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd300d10-ed37-40e5-9b06-c9860fe1c318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f18e19-1780-4856-b5f4-df03604a498d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ec0c05c-a813-4a8e-8721-7421660dcb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>offer_id</th>\n",
       "      <th>id4</th>\n",
       "      <th>id5</th>\n",
       "      <th>y</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>...</th>\n",
       "      <th>cashback_value</th>\n",
       "      <th>offer_duration_months</th>\n",
       "      <th>store_type_encoded</th>\n",
       "      <th>has_cashback</th>\n",
       "      <th>offer_ctr</th>\n",
       "      <th>industry_code</th>\n",
       "      <th>redemption_frequency</th>\n",
       "      <th>industry_total_spent</th>\n",
       "      <th>industry_avg_spent</th>\n",
       "      <th>industry_txn_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1366776_189706075_16-23_2023-11-02 22:22:00.042</td>\n",
       "      <td>1366776</td>\n",
       "      <td>189706075</td>\n",
       "      <td>2023-11-02 22:22:00.042</td>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.059875</td>\n",
       "      <td>57310000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16249283.79</td>\n",
       "      <td>433.638017</td>\n",
       "      <td>37472.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1366776_89227_16-23_2023-11-01 23:51:24.999</td>\n",
       "      <td>1366776</td>\n",
       "      <td>89227</td>\n",
       "      <td>2023-11-01 23:51:24.999</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046487</td>\n",
       "      <td>59210000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3511750.83</td>\n",
       "      <td>142.771510</td>\n",
       "      <td>24597.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1366776_35046_16-23_2023-11-01 00:30:59.797</td>\n",
       "      <td>1366776</td>\n",
       "      <td>35046</td>\n",
       "      <td>2023-11-01 00:30:59.797</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.041484</td>\n",
       "      <td>72310000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2659689.37</td>\n",
       "      <td>137.033818</td>\n",
       "      <td>19409.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1366776_6275451_16-23_2023-11-02 22:21:32.261</td>\n",
       "      <td>1366776</td>\n",
       "      <td>6275451</td>\n",
       "      <td>2023-11-02 22:21:32.261</td>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042805</td>\n",
       "      <td>56510500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14216414.45</td>\n",
       "      <td>249.131054</td>\n",
       "      <td>57064.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1366776_78053_16-23_2023-11-02 22:21:34.799</td>\n",
       "      <td>1366776</td>\n",
       "      <td>78053</td>\n",
       "      <td>2023-11-02 22:21:34.799</td>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042544</td>\n",
       "      <td>59991300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2352086.84</td>\n",
       "      <td>167.946222</td>\n",
       "      <td>14005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770159</th>\n",
       "      <td>1896641_87731_16-23_2023-11-02 08:14:21.524</td>\n",
       "      <td>1896641</td>\n",
       "      <td>87731</td>\n",
       "      <td>2023-11-02 08:14:21.524</td>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011329</td>\n",
       "      <td>59991311.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1153882.46</td>\n",
       "      <td>113.336849</td>\n",
       "      <td>10181.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770160</th>\n",
       "      <td>1896641_505604_16-23_2023-11-02 08:14:24.458</td>\n",
       "      <td>1896641</td>\n",
       "      <td>505604</td>\n",
       "      <td>2023-11-02 08:14:24.458</td>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010578</td>\n",
       "      <td>70110000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13089274.75</td>\n",
       "      <td>894.137219</td>\n",
       "      <td>14639.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770161</th>\n",
       "      <td>1896641_25212_16-23_2023-11-02 08:14:25.748</td>\n",
       "      <td>1896641</td>\n",
       "      <td>25212</td>\n",
       "      <td>2023-11-02 08:14:25.748</td>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011467</td>\n",
       "      <td>70110000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13089274.75</td>\n",
       "      <td>894.137219</td>\n",
       "      <td>14639.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770162</th>\n",
       "      <td>1900765_95157_16-23_2023-11-02 06:08:25.900</td>\n",
       "      <td>1900765</td>\n",
       "      <td>95157</td>\n",
       "      <td>2023-11-02 06:08:25.900</td>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050302</td>\n",
       "      <td>57190000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2126908.38</td>\n",
       "      <td>371.512381</td>\n",
       "      <td>5725.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>770163</th>\n",
       "      <td>1901215_95807_16-23_2023-11-01 11:01:33.086</td>\n",
       "      <td>1901215</td>\n",
       "      <td>95807</td>\n",
       "      <td>2023-11-01 11:01:33.086</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011225</td>\n",
       "      <td>80491200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1062811.05</td>\n",
       "      <td>372.524027</td>\n",
       "      <td>2853.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>770164 rows × 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    id1  customer_id  \\\n",
       "0       1366776_189706075_16-23_2023-11-02 22:22:00.042      1366776   \n",
       "1           1366776_89227_16-23_2023-11-01 23:51:24.999      1366776   \n",
       "2           1366776_35046_16-23_2023-11-01 00:30:59.797      1366776   \n",
       "3         1366776_6275451_16-23_2023-11-02 22:21:32.261      1366776   \n",
       "4           1366776_78053_16-23_2023-11-02 22:21:34.799      1366776   \n",
       "...                                                 ...          ...   \n",
       "770159      1896641_87731_16-23_2023-11-02 08:14:21.524      1896641   \n",
       "770160     1896641_505604_16-23_2023-11-02 08:14:24.458      1896641   \n",
       "770161      1896641_25212_16-23_2023-11-02 08:14:25.748      1896641   \n",
       "770162      1900765_95157_16-23_2023-11-02 06:08:25.900      1900765   \n",
       "770163      1901215_95807_16-23_2023-11-01 11:01:33.086      1901215   \n",
       "\n",
       "         offer_id                      id4         id5  y   f1  f2  f3  f4  \\\n",
       "0       189706075  2023-11-02 22:22:00.042  2023-11-02  0  1.0 NaN NaN NaN   \n",
       "1           89227  2023-11-01 23:51:24.999  2023-11-01  0  1.0 NaN NaN NaN   \n",
       "2           35046  2023-11-01 00:30:59.797  2023-11-01  0  1.0 NaN NaN NaN   \n",
       "3         6275451  2023-11-02 22:21:32.261  2023-11-02  0  1.0 NaN NaN NaN   \n",
       "4           78053  2023-11-02 22:21:34.799  2023-11-02  0  1.0 NaN NaN NaN   \n",
       "...           ...                      ...         ... ..  ...  ..  ..  ..   \n",
       "770159      87731  2023-11-02 08:14:21.524  2023-11-02  0  NaN NaN NaN NaN   \n",
       "770160     505604  2023-11-02 08:14:24.458  2023-11-02  0  NaN NaN NaN NaN   \n",
       "770161      25212  2023-11-02 08:14:25.748  2023-11-02  0  NaN NaN NaN NaN   \n",
       "770162      95157  2023-11-02 06:08:25.900  2023-11-02  0  NaN NaN NaN NaN   \n",
       "770163      95807  2023-11-01 11:01:33.086  2023-11-01  0  NaN NaN NaN NaN   \n",
       "\n",
       "        ...  cashback_value  offer_duration_months  store_type_encoded  \\\n",
       "0       ...             2.0                    1.0               104.0   \n",
       "1       ...             0.0                    6.0                68.0   \n",
       "2       ...            10.0                    1.0                13.0   \n",
       "3       ...            10.0                    1.0                47.0   \n",
       "4       ...             8.0                    1.0                86.0   \n",
       "...     ...             ...                    ...                 ...   \n",
       "770159  ...             0.0                    3.0                86.0   \n",
       "770160  ...             0.0                    6.0                60.0   \n",
       "770161  ...             0.0                    4.0                60.0   \n",
       "770162  ...             0.0                    4.0                83.0   \n",
       "770163  ...             0.0                    4.0                93.0   \n",
       "\n",
       "        has_cashback  offer_ctr  industry_code  redemption_frequency  \\\n",
       "0                1.0   0.059875     57310000.0                   2.0   \n",
       "1                0.0   0.046487     59210000.0                   2.0   \n",
       "2                1.0   0.041484     72310000.0                   2.0   \n",
       "3                1.0   0.042805     56510500.0                   2.0   \n",
       "4                1.0   0.042544     59991300.0                   2.0   \n",
       "...              ...        ...            ...                   ...   \n",
       "770159           0.0   0.011329     59991311.0                   1.0   \n",
       "770160           0.0   0.010578     70110000.0                   1.0   \n",
       "770161           0.0   0.011467     70110000.0                   1.0   \n",
       "770162           0.0   0.050302     57190000.0                   1.0   \n",
       "770163           0.0   0.011225     80491200.0                   1.0   \n",
       "\n",
       "        industry_total_spent  industry_avg_spent  industry_txn_count  \n",
       "0                16249283.79          433.638017             37472.0  \n",
       "1                 3511750.83          142.771510             24597.0  \n",
       "2                 2659689.37          137.033818             19409.0  \n",
       "3                14216414.45          249.131054             57064.0  \n",
       "4                 2352086.84          167.946222             14005.0  \n",
       "...                      ...                 ...                 ...  \n",
       "770159            1153882.46          113.336849             10181.0  \n",
       "770160           13089274.75          894.137219             14639.0  \n",
       "770161           13089274.75          894.137219             14639.0  \n",
       "770162            2126908.38          371.512381              5725.0  \n",
       "770163            1062811.05          372.524027              2853.0  \n",
       "\n",
       "[770164 rows x 382 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "924c01b7-49db-4ed9-9b2b-78c5de32a776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>id3</th>\n",
       "      <th>id4</th>\n",
       "      <th>id5</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>...</th>\n",
       "      <th>cashback_value</th>\n",
       "      <th>offer_duration_months</th>\n",
       "      <th>store_type_encoded</th>\n",
       "      <th>has_cashback</th>\n",
       "      <th>offer_ctr</th>\n",
       "      <th>industry_code</th>\n",
       "      <th>redemption_frequency</th>\n",
       "      <th>industry_total_spent</th>\n",
       "      <th>industry_avg_spent</th>\n",
       "      <th>industry_txn_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1362907_91950_16-23_2023-11-04 18:56:26.000794</td>\n",
       "      <td>1362907</td>\n",
       "      <td>91950</td>\n",
       "      <td>2023-11-04 18:56:26.000794</td>\n",
       "      <td>2023-11-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007848</td>\n",
       "      <td>56619906.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101622.31</td>\n",
       "      <td>249.074289</td>\n",
       "      <td>408.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1082599_88356_16-23_2023-11-04 06:08:53.373</td>\n",
       "      <td>1082599</td>\n",
       "      <td>88356</td>\n",
       "      <td>2023-11-04 06:08:53.373</td>\n",
       "      <td>2023-11-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011145</td>\n",
       "      <td>59440109.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1064140.11</td>\n",
       "      <td>3399.808658</td>\n",
       "      <td>313.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1888466_958700_16-23_2023-11-05 10:07:28.000725</td>\n",
       "      <td>1888466</td>\n",
       "      <td>958700</td>\n",
       "      <td>2023-11-05 10:07:28.000725</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067623</td>\n",
       "      <td>59990000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14203022.21</td>\n",
       "      <td>207.631346</td>\n",
       "      <td>68405.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1888971_795739_16-23_2023-11-04 12:25:28.244</td>\n",
       "      <td>1888971</td>\n",
       "      <td>795739</td>\n",
       "      <td>2023-11-04 12:25:28.244</td>\n",
       "      <td>2023-11-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004302</td>\n",
       "      <td>56990300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2426407.48</td>\n",
       "      <td>178.032686</td>\n",
       "      <td>13629.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1256369_82296_16-23_2023-11-05 06:45:26.657</td>\n",
       "      <td>1256369</td>\n",
       "      <td>82296</td>\n",
       "      <td>2023-11-05 06:45:26.657</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005019</td>\n",
       "      <td>56990300.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2426407.48</td>\n",
       "      <td>178.032686</td>\n",
       "      <td>13629.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369296</th>\n",
       "      <td>1874443_95537_16-23_2023-11-05 09:21:24.182</td>\n",
       "      <td>1874443</td>\n",
       "      <td>95537</td>\n",
       "      <td>2023-11-05 09:21:24.182</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>57120000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11092126.60</td>\n",
       "      <td>818.546720</td>\n",
       "      <td>13551.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369297</th>\n",
       "      <td>1541978_5718_16-23_2023-11-05 00:56:43.946</td>\n",
       "      <td>1541978</td>\n",
       "      <td>5718</td>\n",
       "      <td>2023-11-05 00:56:43.946</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007497</td>\n",
       "      <td>72990000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5232229.78</td>\n",
       "      <td>258.203207</td>\n",
       "      <td>20264.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369298</th>\n",
       "      <td>1887841_85905_16-23_2023-11-05 20:40:43.312</td>\n",
       "      <td>1887841</td>\n",
       "      <td>85905</td>\n",
       "      <td>2023-11-05 20:40:43.312</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.054671</td>\n",
       "      <td>59991503.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15502.18</td>\n",
       "      <td>96.888625</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369299</th>\n",
       "      <td>1569367_944713_16-23_2023-11-05 00:43:04.335</td>\n",
       "      <td>1569367</td>\n",
       "      <td>944713</td>\n",
       "      <td>2023-11-05 00:43:04.335</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006531</td>\n",
       "      <td>54990000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4945570.46</td>\n",
       "      <td>81.011179</td>\n",
       "      <td>61048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369300</th>\n",
       "      <td>1086547_60142_16-23_2023-11-05 10:37:36.747</td>\n",
       "      <td>1086547</td>\n",
       "      <td>60142</td>\n",
       "      <td>2023-11-05 10:37:36.747</td>\n",
       "      <td>2023-11-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.080629</td>\n",
       "      <td>54990000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4945570.46</td>\n",
       "      <td>81.011179</td>\n",
       "      <td>61048.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>369301 rows × 381 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    id1      id2     id3  \\\n",
       "0        1362907_91950_16-23_2023-11-04 18:56:26.000794  1362907   91950   \n",
       "1           1082599_88356_16-23_2023-11-04 06:08:53.373  1082599   88356   \n",
       "2       1888466_958700_16-23_2023-11-05 10:07:28.000725  1888466  958700   \n",
       "3          1888971_795739_16-23_2023-11-04 12:25:28.244  1888971  795739   \n",
       "4           1256369_82296_16-23_2023-11-05 06:45:26.657  1256369   82296   \n",
       "...                                                 ...      ...     ...   \n",
       "369296      1874443_95537_16-23_2023-11-05 09:21:24.182  1874443   95537   \n",
       "369297       1541978_5718_16-23_2023-11-05 00:56:43.946  1541978    5718   \n",
       "369298      1887841_85905_16-23_2023-11-05 20:40:43.312  1887841   85905   \n",
       "369299     1569367_944713_16-23_2023-11-05 00:43:04.335  1569367  944713   \n",
       "369300      1086547_60142_16-23_2023-11-05 10:37:36.747  1086547   60142   \n",
       "\n",
       "                               id4         id5    f1    f2   f3  f4    f5  \\\n",
       "0       2023-11-04 18:56:26.000794  2023-11-04   NaN   NaN  NaN NaN   NaN   \n",
       "1          2023-11-04 06:08:53.373  2023-11-04   NaN   9.0  NaN NaN   NaN   \n",
       "2       2023-11-05 10:07:28.000725  2023-11-05   NaN   NaN  NaN NaN  22.0   \n",
       "3          2023-11-04 12:25:28.244  2023-11-04   NaN   NaN  NaN NaN   NaN   \n",
       "4          2023-11-05 06:45:26.657  2023-11-05   NaN   NaN  NaN NaN   NaN   \n",
       "...                            ...         ...   ...   ...  ...  ..   ...   \n",
       "369296     2023-11-05 09:21:24.182  2023-11-05   NaN   NaN  NaN NaN   NaN   \n",
       "369297     2023-11-05 00:56:43.946  2023-11-05  23.0   NaN  1.0 NaN  10.0   \n",
       "369298     2023-11-05 20:40:43.312  2023-11-05   NaN   NaN  NaN NaN   NaN   \n",
       "369299     2023-11-05 00:43:04.335  2023-11-05   NaN   NaN  NaN NaN   NaN   \n",
       "369300     2023-11-05 10:37:36.747  2023-11-05   NaN  28.0  NaN NaN  43.0   \n",
       "\n",
       "        ...  cashback_value  offer_duration_months  store_type_encoded  \\\n",
       "0       ...             0.0                    5.0               113.0   \n",
       "1       ...             0.0                    2.0                64.0   \n",
       "2       ...             0.0                    1.0                86.0   \n",
       "3       ...             0.0                    6.0                80.0   \n",
       "4       ...             0.0                    6.0                80.0   \n",
       "...     ...             ...                    ...                 ...   \n",
       "369296  ...             0.0                    3.0                52.0   \n",
       "369297  ...             0.0                    5.0                84.0   \n",
       "369298  ...             5.0                    1.0                86.0   \n",
       "369299  ...             0.0                    2.0                81.0   \n",
       "369300  ...             0.0                    2.0                81.0   \n",
       "\n",
       "        has_cashback  offer_ctr  industry_code  redemption_frequency  \\\n",
       "0                0.0   0.007848     56619906.0                   1.0   \n",
       "1                0.0   0.011145     59440109.0                   1.0   \n",
       "2                0.0   0.067623     59990000.0                   2.0   \n",
       "3                0.0   0.004302     56990300.0                   2.0   \n",
       "4                0.0   0.005019     56990300.0                   1.0   \n",
       "...              ...        ...            ...                   ...   \n",
       "369296           0.0   0.004840     57120000.0                   1.0   \n",
       "369297           0.0   0.007497     72990000.0                   1.0   \n",
       "369298           1.0   0.054671     59991503.0                   2.0   \n",
       "369299           0.0   0.006531     54990000.0                   2.0   \n",
       "369300           0.0   0.080629     54990000.0                   1.0   \n",
       "\n",
       "        industry_total_spent  industry_avg_spent  industry_txn_count  \n",
       "0                  101622.31          249.074289               408.0  \n",
       "1                 1064140.11         3399.808658               313.0  \n",
       "2                14203022.21          207.631346             68405.0  \n",
       "3                 2426407.48          178.032686             13629.0  \n",
       "4                 2426407.48          178.032686             13629.0  \n",
       "...                      ...                 ...                 ...  \n",
       "369296           11092126.60          818.546720             13551.0  \n",
       "369297            5232229.78          258.203207             20264.0  \n",
       "369298              15502.18           96.888625               160.0  \n",
       "369299            4945570.46           81.011179             61048.0  \n",
       "369300            4945570.46           81.011179             61048.0  \n",
       "\n",
       "[369301 rows x 381 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4668117-5b96-49d3-96e4-a2479ec65c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cols = ['f42', 'f50', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f354']\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in cols:\n",
    "    test_df[col] = le.fit_transform(test_df[col].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c2a4718-7929-4d10-889d-89ca40f45450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f42     int32\n",
       "f50     int32\n",
       "f52     int32\n",
       "f53     int32\n",
       "f54     int32\n",
       "f55     int32\n",
       "f56     int32\n",
       "f57     int32\n",
       "f354    int32\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[cols].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6207a8-f1b6-48bd-90cd-749b6eb1d6de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped columns: ['f4', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f33', 'f34', 'f36', 'f37', 'f64', 'f66', 'f70', 'f79', 'f80', 'f81', 'f84', 'f88', 'f92', 'f112', 'f114', 'f117', 'f118', 'f120', 'f122', 'f135', 'f136', 'f154', 'f176', 'f189', 'f205', 'f220', 'f221', 'f360']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Calculate missing percentage per column\n",
    "missing_percent = train_df.isnull().mean()\n",
    "\n",
    "# Step 2: Filter columns with >60% missing\n",
    "cols_to_drop = missing_percent[missing_percent > 0.9].index\n",
    "\n",
    "# Step 3: Drop them from both train and test (to ensure same features)\n",
    "train_df = train_df.drop(columns=cols_to_drop)\n",
    "test_df = test_df.drop(columns=cols_to_drop, errors='ignore')  # in case some columns are missing in test\n",
    "\n",
    "print(f\"Dropped columns: {list(cols_to_drop)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78dc2f6d-578a-48be-a901-dddc51937507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaNs in training data with column-wise mean\n",
    "train_df = train_df.fillna(train_df.mean(numeric_only=True))\n",
    "\n",
    "# Fill NaNs in test data with column-wise mean\n",
    "test_df = test_df.fillna(test_df.mean(numeric_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76d3d1ae-5957-4af9-bf14-fe29115a8d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15fa6fc9-3e2d-49b5-95bd-8bd5e48733c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=[ 'id1','customer_id', 'offer_id', 'y', 'id4', 'id5'])\n",
    "y = train_df['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc58beb-b119-4aeb-89bf-d0f0de58e96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Convert to NumPy in chunks (if not already)\n",
    "X_np = X.values if isinstance(X, pd.DataFrame) else X\n",
    "\n",
    "# Fit in chunks\n",
    "chunk_size = 100000  # adjust based on RAM\n",
    "for i in range(0, X_np.shape[0], chunk_size):\n",
    "    scaler.partial_fit(X_np[i:i+chunk_size])\n",
    "\n",
    "# Now transform in chunks\n",
    "X_scaled_parts = []\n",
    "for i in range(0, X_np.shape[0], chunk_size):\n",
    "    chunk_scaled = scaler.transform(X_np[i:i+chunk_size])\n",
    "    X_scaled_parts.append(chunk_scaled)\n",
    "\n",
    "X_scaled = np.vstack(X_scaled_parts)  # Final scaled array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8778ba5-6e75-4eed-bd42-4d67c2e4d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = test_df.drop(columns=[ 'id1',  'id4', 'id5'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97802ee3-4895-43e0-95e8-3075bd9e0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming `Xt` is your DataFrame to scale\n",
    "# Fix variable name confusion: Xt not X\n",
    "Xt = Xt.copy()\n",
    "\n",
    "# Set chunk size (you can increase it if you have more RAM)\n",
    "chunk_size = 10000\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Convert to numpy safely\n",
    "X_npt = Xt.to_numpy()\n",
    "\n",
    "# Fit scaler incrementally in chunks\n",
    "for i in range(0, X_npt.shape[0], chunk_size):\n",
    "    scaler.partial_fit(X_npt[i:i+chunk_size])\n",
    "\n",
    "# Transform in chunks to reduce memory load\n",
    "X_scaled_parts = []\n",
    "for i in range(0, X_npt.shape[0], chunk_size):\n",
    "    chunk_scaled = scaler.transform(X_npt[i:i+chunk_size])\n",
    "    X_scaled_parts.append(chunk_scaled)\n",
    "\n",
    "# Combine all scaled chunks\n",
    "X_scaled = np.vstack(X_scaled_parts)\n",
    "\n",
    "# Optional: convert back to DataFrame\n",
    "Xt_scaled = pd.DataFrame(X_scaled, columns=Xt.columns, index=Xt.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41a4d9f-2f28-45ea-9978-fb58996a5028",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c91dc82-1be4-4cf8-bc37-ae85822a455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Keep 95% variance\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Option 2: Use fixed number of components\n",
    "# pca = PCA(n_components=50)\n",
    "\n",
    "X_pca = pca.fit_transform(X_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8ef3736-6ed9-430d-b97d-640d92816c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PCA reduced dimensions to 185 components.\n",
      "Explained variance ratio (first few): [0.04860793 0.04051125 0.0352026  0.02804256 0.02703355 0.01820265\n",
      " 0.01560003 0.01361465 0.01307646 0.01234732]\n"
     ]
    }
   ],
   "source": [
    "print(f\"✅ PCA reduced dimensions to {X_pca.shape[1]} components.\")\n",
    "print(\"Explained variance ratio (first few):\", pca.explained_variance_ratio_[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf7cade4-0d7a-47d8-a649-8c8e78ac84e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but PCA was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_pca_test = pca.transform(Xt_scaled)  # Use your fitted PCA on test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "614640d0-c553-4eb0-9f68-2aa6c3d8a0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\fahee\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\fahee\\anaconda3\\lib\\site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\fahee\\anaconda3\\lib\\site-packages (from lightgbm) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3520d96-551f-47dd-9da1-ace124078192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ⛳ Use the same rows for scaling + PCA as used in train_df\n",
    "X = train_df.drop(columns=['y', 'id1'])  # Keep id1 separately\n",
    "y = train_df['y']\n",
    "id1 = train_df['id1']\n",
    "\n",
    "# Fill NaNs (just in case)\n",
    "X = X.fillna(X.mean())\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# PCA (retain 95% variance)\n",
    "pca = PCA(n_components=150, random_state=42)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# ✅ Create PCA DataFrame with correct length\n",
    "X_df = pd.DataFrame(X_pca)\n",
    "X_df['y'] = y.values\n",
    "X_df['id1'] = id1.values\n",
    "\n",
    "# 🔀 Split for validation\n",
    "X_train_df, X_val_df = train_test_split(X_df, test_size=0.2, random_state=42, stratify=X_df['y'])\n",
    "\n",
    "X_train = X_train_df.drop(columns=['y', 'id1'])\n",
    "y_train = X_train_df['y']\n",
    "X_val = X_val_df.drop(columns=['y', 'id1'])\n",
    "y_val = X_val_df['y']\n",
    "\n",
    "# LightGBM datasets\n",
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dval = lgb.Dataset(X_val, label=y_val)\n",
    "\n",
    "# LightGBM training parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# 🚀 Train model\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    valid_sets=[dtrain, dval],\n",
    "    valid_names=['train', 'valid'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 🧪 Handle test data\n",
    "X_test = test_df.drop(columns=['id1', 'id2', 'id3', 'id5'], errors='ignore')\n",
    "X_test = X_test.fillna(X_test.mean())\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# 🔮 Predict\n",
    "probs = model.predict(X_test_pca)\n",
    "\n",
    "# Combine with test IDs\n",
    "pred_df = pd.DataFrame({\n",
    "    'id1': test_df['id1'].values,\n",
    "    'id2': test_df['id2'].values,\n",
    "    'id3': test_df['id3'].values,\n",
    "    'id5': test_df['id5'].values,\n",
    "    'raw_pred': probs\n",
    "})\n",
    "\n",
    "# Normalize predictions per customer\n",
    "def normalize_group(df):\n",
    "    if len(df) == 1:\n",
    "        df['pred'] = 1.0\n",
    "    else:\n",
    "        df['pred'] = softmax(df['raw_pred'].values)\n",
    "    return df\n",
    "\n",
    "submission_df = pred_df.groupby('id1', group_keys=False).apply(normalize_group)\n",
    "submission_df = submission_df.drop(columns=['raw_pred'])\n",
    "submission_df = submission_df.sort_values(by=['id1', 'pred'], ascending=[True, False])\n",
    "\n",
    "# 💾 Save final CSV\n",
    "submission_df.to_csv(\"final_submission.csv\", index=False)\n",
    "print(\"✅ Submission file saved: final_submission.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42734a89-958e-4dde-9b0e-346a80042ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa6c968-6d65-46b5-a79c-e36a67f6bda8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48770f14-c69f-4d62-b9aa-057c4c7b6c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6dc81f6f-b0d7-4bfe-8c58-c92985925d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = xgb_clf.predict(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d480eb0b-1c11-4791-9dca-70c5cc39e6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahee\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but PCA was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Drop same ID and target columns\n",
    "X_test = test_df.drop(columns=['id1', 'id2', 'id3', 'id4', 'id5'])  # Drop same columns as train\n",
    "\n",
    "\n",
    "# Drop columns that were removed from train\n",
    "X_test = X_test[X.columns]  # Ensure matching columns\n",
    "\n",
    "\n",
    "\n",
    "# Apply PCA transformation\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d9ebb-7e3d-4fcd-b747-a65f0a5849ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a75f4a7c-e49e-4994-a2fb-7a42a92eb924",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test_df[['id1', 'id2', 'id3', 'id5']].copy()\n",
    "submission['pred'] = y_pred_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5b8f5b62-01db-43c3-ac26-18e179b63fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('final_submission_ammk.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "48718354-6094-4692-bc1c-57822373da52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Submission file 'xgboost_submission.csv' created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predict on the final test PCA data\n",
    "y_pred_test = xgb_clf.predict(X_test_pca)\n",
    "\n",
    "# Create the final submission DataFrame\n",
    "submission = test_df[['id1', 'id2', 'id3', 'id5']].copy()\n",
    "submission['pred'] = y_pred_test\n",
    "\n",
    "# Save to CSV\n",
    "submission.to_csv('xgboost_submission.csv', index=False)\n",
    "print(\"✅ Submission file 'xgboost_submission.csv' created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7f7646-a5dd-410f-a22c-9d890a8fb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Combine X and y back into a single DataFrame\n",
    "df_corr = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Calculate Pearson correlation of each feature with y\n",
    "correlations = df_corr.corr()['y'].drop('y').sort_values(ascending=False)\n",
    "\n",
    "# Display top positively and negatively correlated features\n",
    "print(\"🔍 Top positively correlated features with y:\")\n",
    "print(correlations.head(10))\n",
    "\n",
    "print(\"\\n🔍 Top negatively correlated features with y:\")\n",
    "print(correlations.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d5ca6957-343c-449a-8b6b-700f406e5383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Validation Accuracy with PCA: 0.9645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "score = model.score(X_val, y_val)\n",
    "print(f\"📊 Validation Accuracy with PCA: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ac9decb-043b-4a22-8e71-a7e1d4feed17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d65de331-b8a9-40f3-a1f1-bd8fd73eb129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:06:58] WARNING: /Users/runner/work/xgboost/xgboost/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              feature_weights=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
       "              n_jobs=None, num_parallel_tree=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;XGBClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://xgboost.readthedocs.io/en/release_3.0.0/python/python_api.html#xgboost.XGBClassifier\">?<span>Documentation for XGBClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              feature_weights=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
       "              n_jobs=None, num_parallel_tree=None, ...)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='auc', feature_types=None,\n",
       "              feature_weights=None, gamma=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.1, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=None, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=100,\n",
       "              n_jobs=None, num_parallel_tree=None, ...)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='auc',\n",
    "    use_label_encoder=False,\n",
    "    n_estimators=100,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d0dbec9-dd41-4fde-9ce4-2e59056ece2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_proba = model.predict_proba(X_valid)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2365bc56-8ae2-4472-9164-893f771ca474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 0.9476\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc_score = roc_auc_score(y_valid, y_valid_proba)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "467fa48c-0825-4cd3-b5b1-03395ce5eb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_df.drop(columns=['id1', 'id2', 'id3', 'id4', 'id5'])  # Drop same columns as train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f3568f4-fdb2-431a-a243-afdef8b88fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_probs = model.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b945f99-59c1-4d6d-aa07-763abdcd54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['pred'] = test_probs  # or test_preds\n",
    "\n",
    "test_df[['id1', 'id2', 'id3', 'id5', 'pred']].to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303ff45-c752-4578-9ba0-e25ae1e44984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d480277-1b1a-498e-a223-58da107e3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.special import softmax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c3bee82-add0-4537-8cd0-677534ff3d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "test_df = pd.read_csv(\"test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ad49f51-000a-44f7-af1e-2577f9e5fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['f42', 'f50', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f354']\n",
    "le_dict = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train_df[col], test_df[col]], axis=0).astype(str)\n",
    "    le.fit(combined)\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    le_dict[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b83e8cda-7bd7-47d0-ae14-d02ac5e8870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.95\n",
    "missing_ratio = train_df.isnull().mean()\n",
    "cols_to_drop = missing_ratio[missing_ratio > threshold].index\n",
    "train_df = train_df.drop(columns=cols_to_drop)\n",
    "test_df = test_df.drop(columns=cols_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c58f9778-ceb9-4a9f-9846-2851d456db5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.fillna(train_df.mean(numeric_only=True))\n",
    "test_df = test_df.fillna(train_df.mean(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a788cd87-e7be-4682-8d49-fce9fcb20082",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['id1', 'customer_id', 'offer_id', 'y', 'id4', 'id5'])\n",
    "y = train_df['y']\n",
    "X_test = test_df.drop(columns=['id1', 'customer_id', 'offer_id', 'y', 'id2', 'id3', 'id4', 'id5'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "395f5ea1-660a-45ec-9aa1-1859fadd2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abe34320-b0ff-4592-9b30-1cc9f23ee915",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffbf3003-5f47-4946-a51f-a6de2a1a3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = pd.DataFrame(X_pca)\n",
    "X_df['y'] = y\n",
    "X_df['id2'] = train_df['id2'].values if 'id2' in train_df.columns else np.nan\n",
    "\n",
    "X_train_df, X_val_df = train_test_split(X_df, test_size=0.2, random_state=42, stratify=X_df['y'])\n",
    "\n",
    "X_train = X_train_df.drop(columns=['y', 'id2'])\n",
    "y_train = X_train_df['y']\n",
    "X_val = X_val_df.drop(columns=['y', 'id2'])\n",
    "y_val = X_val_df['y']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "237ff181-5eaa-484f-bc6b-2f199c532b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(X_train, label=y_train)\n",
    "dval = lgb.Dataset(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5f03741-d60e-4776-8ee5-12d5601797c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[100]\ttrain's binary_logloss: 0.104132\tvalid's binary_logloss: 0.108314\n",
      "[200]\ttrain's binary_logloss: 0.0938772\tvalid's binary_logloss: 0.100906\n",
      "[300]\ttrain's binary_logloss: 0.0876476\tvalid's binary_logloss: 0.0977719\n",
      "[400]\ttrain's binary_logloss: 0.0826093\tvalid's binary_logloss: 0.0959236\n",
      "[500]\ttrain's binary_logloss: 0.0783759\tvalid's binary_logloss: 0.0946556\n",
      "[600]\ttrain's binary_logloss: 0.0746113\tvalid's binary_logloss: 0.0938251\n",
      "[700]\ttrain's binary_logloss: 0.0709921\tvalid's binary_logloss: 0.0929682\n",
      "[800]\ttrain's binary_logloss: 0.0677423\tvalid's binary_logloss: 0.0922987\n",
      "[900]\ttrain's binary_logloss: 0.0647444\tvalid's binary_logloss: 0.0916292\n",
      "[1000]\ttrain's binary_logloss: 0.0619595\tvalid's binary_logloss: 0.0911336\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\ttrain's binary_logloss: 0.0619595\tvalid's binary_logloss: 0.0911336\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    valid_sets=[dtrain, dval],\n",
    "    valid_names=['train', 'valid'],\n",
    "    num_boost_round=1000,\n",
    "    callbacks=[\n",
    "        lgb.early_stopping(stopping_rounds=50),\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2893309-3f3c-4afb-b2ae-be83bd316cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahee\\AppData\\Local\\Temp\\ipykernel_22916\\2563126803.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df['raw_pred'] = probs\n",
      "C:\\Users\\fahee\\AppData\\Local\\Temp\\ipykernel_22916\\2563126803.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df['rank'] = test_df.groupby('id2')['raw_pred'].rank(method='first', ascending=False)\n",
      "C:\\Users\\fahee\\AppData\\Local\\Temp\\ipykernel_22916\\2563126803.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df['inv_rank'] = test_df.groupby('id2')['rank'].transform(lambda r: 1 / r)\n",
      "C:\\Users\\fahee\\AppData\\Local\\Temp\\ipykernel_22916\\2563126803.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  test_df['pred'] = test_df.groupby('id2')['inv_rank'].transform(lambda x: x / x.sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final submission file saved as 'final_submission_ranked_pca.csv'\n"
     ]
    }
   ],
   "source": [
    "probs = model.predict(X_test_pca)\n",
    "test_df['raw_pred'] = probs\n",
    "test_df['rank'] = test_df.groupby('id2')['raw_pred'].rank(method='first', ascending=False)\n",
    "test_df['inv_rank'] = test_df.groupby('id2')['rank'].transform(lambda r: 1 / r)\n",
    "test_df['pred'] = test_df.groupby('id2')['inv_rank'].transform(lambda x: x / x.sum())\n",
    "\n",
    "# --- Step 11: Save final submission (drop id4) ---\n",
    "submission_df = test_df[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "submission_df.to_csv(\"final_submission_ranked_pca.csv\", index=False)\n",
    "\n",
    "print(\"✅ Final submission file saved as 'final_submission_ranked_pca.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb07524-bd80-45f9-a92d-ad8fdf8b75fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc81a564-20ba-42ee-8a44-b7966029d6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 37051, number of negative: 733113\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.319112 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 48871\n",
      "[LightGBM] [Info] Number of data points in the train set: 770164, number of used features: 372\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.048108 -> initscore=-2.985005\n",
      "[LightGBM] [Info] Start training from score -2.985005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahee\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [18:56:28] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# --- Step 1: Load data ---\n",
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "test_df = pd.read_csv(\"test_processed.csv\")\n",
    "\n",
    "# --- Step 2: Encode categorical features ---\n",
    "categorical_cols = ['f42', 'f50', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f354']\n",
    "le_dict = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train_df[col], test_df[col]], axis=0).astype(str)\n",
    "    le.fit(combined)\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    le_dict[col] = le\n",
    "\n",
    "# --- Step 3: Drop columns with >60% missing values ---\n",
    "\n",
    "\n",
    "# --- Step 4: Fill missing values with mean ---\n",
    "\n",
    "# --- Step 5: Prepare features and target ---\n",
    "X = train_df.drop(columns=['id1', 'customer_id', 'offer_id', 'y', 'id4', 'id5'])\n",
    "y = train_df['y']\n",
    "X_test = test_df.drop(columns=['id1', 'customer_id', 'offer_id', 'y', 'id2', 'id3', 'id4', 'id5'], errors='ignore')\n",
    "\n",
    "# --- Step 6: Train models ---\n",
    "# LightGBM\n",
    "lgb_model = lgb.LGBMClassifier(objective='binary', learning_rate=0.05, num_leaves=31, n_estimators=100)\n",
    "lgb_model.fit(X, y)\n",
    "lgb_preds = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X, y)\n",
    "xgb_preds = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "rf_preds = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- Step 7: Ensemble predictions ---\n",
    "ensemble_probs = (lgb_preds + xgb_preds + rf_preds) / 3\n",
    "\n",
    "# --- Step 8: Rank and normalize ---\n",
    "test_df['raw_pred'] = ensemble_probs\n",
    "test_df['rank'] = test_df.groupby('id2')['raw_pred'].rank(method='first', ascending=False)\n",
    "test_df['inv_rank'] = test_df.groupby('id2')['rank'].transform(lambda r: 1 / r)\n",
    "test_df['pred'] = test_df.groupby('id2')['inv_rank'].transform(lambda x: x / x.sum())\n",
    "\n",
    "# --- Step 9: Save final submission (drop id4) ---\n",
    "submission_df = test_df[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "submission_df.to_csv(\"final_submission_ensemble.csv\", index=False)\n",
    "\n",
    "print(\"✅ Final submission file saved as 'final_submission_ensemble.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c21b2de-8797-4a8b-a628-0a7ab32f3c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b334fda-2704-47d1-a7f1-2c1babd50d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# --- Step 1: Load data ---\n",
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "test_df = pd.read_csv(\"test_processed.csv\")\n",
    "\n",
    "# --- Step 2: Encode categorical features ---\n",
    "categorical_cols = ['f42', 'f50', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f354']\n",
    "le_dict = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train_df[col], test_df[col]], axis=0).astype(str)\n",
    "    le.fit(combined)\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    le_dict[col] = le\n",
    "\n",
    "# --- Step 3: Drop columns with >60% missing values ---\n",
    "\n",
    "\n",
    "# --- Step 5: Prepare features and target ---\n",
    "X = train_df.drop(columns=['id1', 'customer_id', 'offer_id', 'y', 'id4', 'id5'])\n",
    "y = train_df['y']\n",
    "X_test = test_df.drop(columns=['id1', 'customer_id', 'offer_id', 'y', 'id2', 'id3', 'id4', 'id5'], errors='ignore')\n",
    "\n",
    "# --- Step 6: Train-validation split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# --- Step 7: Define base models ---\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=300, learning_rate=0.05, num_leaves=31)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=300, learning_rate=0.05, use_label_encoder=False, eval_metric='logloss')\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "\n",
    "# --- Step 8: Define meta-model ---\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# --- Step 9: Stacking Classifier ---\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lgb', lgb_model),\n",
    "        ('xgb', xgb_model),\n",
    "        ('rf', rf_model)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- Step 10: Train the ensemble ---\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Step 11: Predict ---\n",
    "probs = stack_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- Step 12: Post-process predictions ---\n",
    "test_df['raw_pred'] = probs\n",
    "test_df['rank'] = test_df.groupby('id2')['raw_pred'].rank(method='first', ascending=False)\n",
    "test_df['inv_rank'] = test_df.groupby('id2')['rank'].transform(lambda r: 1 / r)\n",
    "test_df['pred'] = test_df.groupby('id2')['inv_rank'].transform(lambda x: x / x.sum())\n",
    "\n",
    "# --- Step 13: Save final submission (drop id4) ---\n",
    "submission_df = test_df[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "submission_df.to_csv(\"final_submission_stacked.csv\", index=False)\n",
    "\n",
    "print(\"✅ Final stacked submission file saved as 'final_submission_stacked.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d7fac67-5563-4ef2-a5b5-39c83a07d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29cb1f55-5606-487e-a6b0-95a1a7c2cc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "test_df = pd.read_csv(\"test_processed.csv\")\n",
    "\n",
    "# --- Step 2: Encode categorical features ---\n",
    "categorical_cols = ['f42', 'f50', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f354']\n",
    "le_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "168d562a-79ae-4815-84d6-7e77d5802faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train_df[col], test_df[col]], axis=0).astype(str)\n",
    "    le.fit(combined)\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    le_dict[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bc156d5-27e1-4efc-b233-83bdb73eb75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['id1', 'customer_id', 'offer_id', 'y', 'id4', 'id5'])\n",
    "y = train_df['y']\n",
    "X_test = test_df.drop(columns=['id1', 'customer_id', 'offer_id', 'y', 'id2', 'id3', 'id4', 'id5'], errors='ignore')\n",
    "\n",
    "# --- Step 6: Train-validation split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c1521a-570f-4d7b-a698-666c63863630",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model = lgb.LGBMClassifier(n_estimators=300, learning_rate=0.05, num_leaves=31)\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=300, learning_rate=0.05, eval_metric='logloss')\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "\n",
    "# --- Step 8: Define meta-model ---\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# --- Step 9: Stacking Classifier ---\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=[\n",
    "        ('lgb', lgb_model),\n",
    "        ('xgb', xgb_model),\n",
    "        ('rf', rf_model)\n",
    "    ],\n",
    "    final_estimator=meta_model,\n",
    "    passthrough=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# --- Step 10: Train the ensemble ---\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# --- Step 11: Predict ---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3c51e7-0089-4fed-b57e-db4b31463485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 11: Predict ---\n",
    "probs = stack_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# --- Step 12: Post-process predictions ---\n",
    "test_df['raw_pred'] = probs\n",
    "test_df['rank'] = test_df.groupby('id2')['raw_pred'].rank(method='first', ascending=False)\n",
    "test_df['inv_rank'] = test_df.groupby('id2')['rank'].transform(lambda r: 1 / r)\n",
    "test_df['pred'] = test_df.groupby('id2')['inv_rank'].transform(lambda x: x / x.sum())\n",
    "\n",
    "# --- Step 13: Save final submission (drop id4) ---\n",
    "submission_df = test_df[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "submission_df.to_csv(\"final_submission_stacked.csv\", index=False)\n",
    "\n",
    "print(\"✅ Final stacked submission file saved as 'final_submission_stacked.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7d82a2-c794-49db-a61f-c4d9948eb76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c202d08-9560-46f0-b2e7-da38ba791a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159b7425-b828-4f3a-866d-78b68781af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Load data ---\n",
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "test_df = pd.read_csv(\"test_processed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da3a2981-6b2b-4cd9-a0fe-3312fd27ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['f42', 'f50', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f354']\n",
    "le_dict = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train_df[col], test_df[col]], axis=0).astype(str)\n",
    "    le.fit(combined)\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    le_dict[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec095534-dd17-4e72-9c4c-98d41836617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['id1', 'customer_id', 'offer_id', 'y', 'id4', 'id5'])\n",
    "y = train_df['y']\n",
    "X_test = test_df.drop(columns=['id1', 'customer_id', 'offer_id', 'y', 'id2', 'id3', 'id4', 'id5'], errors='ignore')\n",
    "\n",
    "# --- Step 6: Split for classifiers ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "205cd3ba-a5fb-40c8-9e4e-a99ca51088ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 29641, number of negative: 586490\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.764949 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 48794\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 334\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.048108 -> initscore=-2.984997\n",
      "[LightGBM] [Info] Start training from score -2.984997\n"
     ]
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier(n_estimators=300, learning_rate=0.05, num_leaves=31)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "probs_lgb = lgb_model.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "618f092a-982e-47a2-9f28-c886eb4736b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahee\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:12:56] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(n_estimators=300, learning_rate=0.05, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "probs_xgb = xgb_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83cc1a83-0627-432a-a5be-b209f65f4767",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "probs_rf = rf_model.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a52f938-f1d6-4fd9-88cf-758253d26f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 8: Ensemble Predictions (weighted average) ---\n",
    "test_df['raw_pred'] = (\n",
    "    0.4 * probs_lgb +\n",
    "    0.3 * probs_xgb +\n",
    "    0.3 * probs_rf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7366d374-0fe4-4854-8a9f-f6b0f295f549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 9: MAP@7 Style Ranking ---\n",
    "test_df['rank'] = test_df.groupby('id2')['raw_pred'].rank(method='first', ascending=False)\n",
    "test_df['inv_rank'] = test_df.groupby('id2')['rank'].transform(lambda r: 1 / r)\n",
    "test_df['pred'] = test_df.groupby('id2')['inv_rank'].transform(lambda x: x / x.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f1b8e92-3233-4cad-acf1-8aba7f42ae1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ensemble submission saved as 'final_submission_ensemble.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Step 10: Final submission (drop id4) ---\n",
    "submission_df = test_df[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "submission_df.to_csv(\"final_submission_ensemble.csv\", index=False)\n",
    "print(\"✅ Ensemble submission saved as 'final_submission_ensemble.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c607d6d-16a5-4679-ba75-f068180f8901",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahee\\AppData\\Local\\Temp\\ipykernel_23360\\563490755.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  predictions = grouped.apply(lambda x: x.sort_values(\"pred\", ascending=False)[\"id1\"].tolist()).tolist()\n",
      "C:\\Users\\fahee\\AppData\\Local\\Temp\\ipykernel_23360\\563490755.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  actuals = grouped.apply(lambda x: [x.loc[x[\"pred\"].idxmax(), \"id1\"]]).tolist()\n",
      "C:\\Users\\fahee\\AppData\\Local\\Temp\\ipykernel_23360\\563490755.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  predictions = grouped.apply(lambda x: x.sort_values(\"pred\", ascending=False)[\"id1\"].tolist()).tolist()\n",
      "C:\\Users\\fahee\\AppData\\Local\\Temp\\ipykernel_23360\\563490755.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  actuals = grouped.apply(lambda x: [x.loc[x[\"pred\"].idxmax(), \"id1\"]]).tolist()\n",
      "C:\\Users\\fahee\\AppData\\Local\\Temp\\ipykernel_23360\\563490755.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  predictions = grouped.apply(lambda x: x.sort_values(\"pred\", ascending=False)[\"id1\"].tolist()).tolist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@7 Ensemble: 1.0\n",
      "MAP@7 Ranked: 0.884894853859718\n",
      "MAP@7 PCA: 0.7525573985631756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahee\\AppData\\Local\\Temp\\ipykernel_23360\\563490755.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  actuals = grouped.apply(lambda x: [x.loc[x[\"pred\"].idxmax(), \"id1\"]]).tolist()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def apk(actual, predicted, k=7):\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            hits += 1.0\n",
    "            score += hits / (i + 1.0)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mapk(actual, predicted, k=7):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actual, predicted)])\n",
    "\n",
    "def prepare_map_data(df):\n",
    "    grouped = df.groupby(\"id2\")\n",
    "    predictions = grouped.apply(lambda x: x.sort_values(\"pred\", ascending=False)[\"id1\"].tolist()).tolist()\n",
    "    actuals = grouped.apply(lambda x: [x.loc[x[\"pred\"].idxmax(), \"id1\"]]).tolist()\n",
    "    return actuals, predictions\n",
    "\n",
    "# Load CSVs\n",
    "ensemble_df = pd.read_csv(\"final_submission_ensemble.csv\")\n",
    "ranked_df = pd.read_csv(\"final_submission_rankedfinalfinal.csv\")\n",
    "ranked_pca_df = pd.read_csv(\"final_submission_ranked_pca.csv\")\n",
    "\n",
    "# Prepare and score\n",
    "actuals, pred_ensemble = prepare_map_data(ensemble_df)\n",
    "_, pred_ranked = prepare_map_data(ranked_df)\n",
    "_, pred_pca = prepare_map_data(ranked_pca_df)\n",
    "\n",
    "print(\"MAP@7 Ensemble:\", mapk(actuals, pred_ensemble))\n",
    "print(\"MAP@7 Ranked:\", mapk(actuals, pred_ranked))\n",
    "print(\"MAP@7 PCA:\", mapk(actuals, pred_pca))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95da910c-4d85-4167-bacc-92c8cc042d46",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_processed.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# --- Step 2: Create ground truth dictionary ---\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Only those offers that were actually accepted (y = 1), grouped by id2\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m train_df[train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid2\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# --- Step 3: Load prediction submissions ---\u001b[39;00m\n\u001b[0;32m     29\u001b[0m ensemble_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_submission_ensemble.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   9184\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9185\u001b[0m     keys\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m   9186\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   9187\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   9188\u001b[0m     as_index\u001b[38;5;241m=\u001b[39mas_index,\n\u001b[0;32m   9189\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   9190\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[0;32m   9191\u001b[0m     observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m   9192\u001b[0m     dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[0;32m   9193\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m get_grouper(\n\u001b[0;32m   1330\u001b[0m         obj,\n\u001b[0;32m   1331\u001b[0m         keys,\n\u001b[0;32m   1332\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   1333\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   1334\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   1335\u001b[0m         observed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m observed,\n\u001b[0;32m   1336\u001b[0m         dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna,\n\u001b[0;32m   1337\u001b[0m     )\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id2'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def mapk(y_true_dict, y_pred_dict, k=7):\n",
    "    \"\"\"Compute Mean Average Precision at k\"\"\"\n",
    "    map_total = 0\n",
    "    for cust_id in y_true_dict:\n",
    "        actual = y_true_dict[cust_id]\n",
    "        pred = y_pred_dict.get(cust_id, [])[:k]\n",
    "        score = 0.0\n",
    "        hits = 0.0\n",
    "        for i, p in enumerate(pred):\n",
    "            if p in actual and p not in pred[:i]:\n",
    "                hits += 1.0\n",
    "                score += hits / (i + 1.0)\n",
    "        if len(actual) == 0:\n",
    "            continue\n",
    "        map_total += score / min(len(actual), k)\n",
    "    return map_total / len(y_true_dict)\n",
    "\n",
    "\n",
    "# --- Step 1: Load data ---\n",
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "\n",
    "# --- Step 2: Create ground truth dictionary ---\n",
    "# Only those offers that were actually accepted (y = 1), grouped by id2\n",
    "ground_truth = train_df[train_df['y'] == 1].groupby('id2')['id1'].apply(list).to_dict()\n",
    "\n",
    "# --- Step 3: Load prediction submissions ---\n",
    "ensemble_df = pd.read_csv(\"final_submission_ensemble.csv\")\n",
    "ranked_df = pd.read_csv(\"final_submission_rankedfinalfinal.csv\")\n",
    "pca_df = pd.read_csv(\"final_submission_ranked_pca.csv\")\n",
    "\n",
    "# --- Step 4: Create prediction dictionaries ---\n",
    "ensemble_preds = ensemble_df.groupby('id2')['id1'].apply(list).to_dict()\n",
    "ranked_preds = ranked_df.groupby('id2')['id1'].apply(list).to_dict()\n",
    "pca_preds = pca_df.groupby('id2')['id1'].apply(list).to_dict()\n",
    "\n",
    "# --- Step 5: Calculate MAP@7 for each ---\n",
    "score_ensemble = mapk(ground_truth, ensemble_preds, k=7)\n",
    "score_ranked = mapk(ground_truth, ranked_preds, k=7)\n",
    "score_pca = mapk(ground_truth, pca_preds, k=7)\n",
    "\n",
    "# --- Step 6: Print results ---\n",
    "print(f\"✅ MAP@7 Ensemble: {score_ensemble:.6f}\")\n",
    "print(f\"✅ MAP@7 Ranked:   {score_ranked:.6f}\")\n",
    "print(f\"✅ MAP@7 PCA:      {score_pca:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf49db31-126e-4032-99a1-9e767c99ce9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'customer_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m pca_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_submission_ranked_pca.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# --- Prediction dictionaries using customer_id ---\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m ensemble_preds \u001b[38;5;241m=\u001b[39m ensemble_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m     33\u001b[0m ranked_preds \u001b[38;5;241m=\u001b[39m ranked_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m     34\u001b[0m pca_preds \u001b[38;5;241m=\u001b[39m pca_df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   9184\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9185\u001b[0m     keys\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m   9186\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   9187\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   9188\u001b[0m     as_index\u001b[38;5;241m=\u001b[39mas_index,\n\u001b[0;32m   9189\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   9190\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[0;32m   9191\u001b[0m     observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m   9192\u001b[0m     dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[0;32m   9193\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m get_grouper(\n\u001b[0;32m   1330\u001b[0m         obj,\n\u001b[0;32m   1331\u001b[0m         keys,\n\u001b[0;32m   1332\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   1333\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   1334\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   1335\u001b[0m         observed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m observed,\n\u001b[0;32m   1336\u001b[0m         dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna,\n\u001b[0;32m   1337\u001b[0m     )\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'customer_id'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def mapk(y_true_dict, y_pred_dict, k=7):\n",
    "    map_total = 0\n",
    "    for cust_id in y_true_dict:\n",
    "        actual = y_true_dict[cust_id]\n",
    "        pred = y_pred_dict.get(cust_id, [])[:k]\n",
    "        score = 0.0\n",
    "        hits = 0.0\n",
    "        for i, p in enumerate(pred):\n",
    "            if p in actual and p not in pred[:i]:\n",
    "                hits += 1.0\n",
    "                score += hits / (i + 1.0)\n",
    "        if len(actual) == 0:\n",
    "            continue\n",
    "        map_total += score / min(len(actual), k)\n",
    "    return map_total / len(y_true_dict)\n",
    "\n",
    "\n",
    "# --- Load data ---\n",
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "\n",
    "# --- Use customer_id instead of id2 ---\n",
    "ground_truth = train_df[train_df['y'] == 1].groupby('customer_id')['id1'].apply(list).to_dict()\n",
    "\n",
    "# --- Load submissions ---\n",
    "ensemble_df = pd.read_csv(\"final_submission_ensemble.csv\")\n",
    "ranked_df = pd.read_csv(\"final_submission_rankedfinalfinal.csv\")\n",
    "pca_df = pd.read_csv(\"final_submission_ranked_pca.csv\")\n",
    "\n",
    "# --- Prediction dictionaries using customer_id ---\n",
    "ensemble_preds = ensemble_df.groupby('customer_id')['id1'].apply(list).to_dict()\n",
    "ranked_preds = ranked_df.groupby('customer_id')['id1'].apply(list).to_dict()\n",
    "pca_preds = pca_df.groupby('customer_id')['id1'].apply(list).to_dict()\n",
    "\n",
    "# --- Compute MAP@7 ---\n",
    "score_ensemble = mapk(ground_truth, ensemble_preds, k=7)\n",
    "score_ranked = mapk(ground_truth, ranked_preds, k=7)\n",
    "score_pca = mapk(ground_truth, pca_preds, k=7)\n",
    "\n",
    "# --- Output ---\n",
    "print(f\"✅ MAP@7 Ensemble: {score_ensemble:.6f}\")\n",
    "print(f\"✅ MAP@7 Ranked:   {score_ranked:.6f}\")\n",
    "print(f\"✅ MAP@7 PCA:      {score_pca:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5c434a9-c7dc-49ae-8d7b-988081fea909",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'customer_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m pca_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_submission_ranked_pca.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# --- Step 4: Use same grouping key in predictions ---\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m ensemble_preds \u001b[38;5;241m=\u001b[39m ensemble_df\u001b[38;5;241m.\u001b[39mgroupby(group_col)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m     19\u001b[0m ranked_preds \u001b[38;5;241m=\u001b[39m ranked_df\u001b[38;5;241m.\u001b[39mgroupby(group_col)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m     20\u001b[0m pca_preds \u001b[38;5;241m=\u001b[39m pca_df\u001b[38;5;241m.\u001b[39mgroupby(group_col)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[1;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[0;32m   9184\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9185\u001b[0m     keys\u001b[38;5;241m=\u001b[39mby,\n\u001b[0;32m   9186\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   9187\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   9188\u001b[0m     as_index\u001b[38;5;241m=\u001b[39mas_index,\n\u001b[0;32m   9189\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   9190\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[0;32m   9191\u001b[0m     observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[0;32m   9192\u001b[0m     dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[0;32m   9193\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[1;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m get_grouper(\n\u001b[0;32m   1330\u001b[0m         obj,\n\u001b[0;32m   1331\u001b[0m         keys,\n\u001b[0;32m   1332\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   1333\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   1334\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[0;32m   1335\u001b[0m         observed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m observed,\n\u001b[0;32m   1336\u001b[0m         dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna,\n\u001b[0;32m   1337\u001b[0m     )\n\u001b[0;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\groupby\\grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[1;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[0;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'customer_id'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Load training data to get ground truth ---\n",
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "\n",
    "# Use 'id2' if 'customer_id' doesn't exist\n",
    "group_col = 'customer_id' if 'customer_id' in train_df.columns else 'id2'\n",
    "\n",
    "# --- Step 2: Create ground truth dictionary ---\n",
    "ground_truth = train_df[train_df['y'] == 1].groupby(group_col)['id1'].apply(list).to_dict()\n",
    "\n",
    "# --- Step 3: Load prediction submission files ---\n",
    "ensemble_df = pd.read_csv(\"final_submission_ensemble.csv\")\n",
    "ranked_df = pd.read_csv(\"final_submission_rankedfinalfinal.csv\")\n",
    "pca_df = pd.read_csv(\"final_submission_ranked_pca.csv\")\n",
    "\n",
    "# --- Step 4: Use same grouping key in predictions ---\n",
    "ensemble_preds = ensemble_df.groupby(group_col)['id1'].apply(list).to_dict()\n",
    "ranked_preds = ranked_df.groupby(group_col)['id1'].apply(list).to_dict()\n",
    "pca_preds = pca_df.groupby(group_col)['id1'].apply(list).to_dict()\n",
    "\n",
    "# --- Step 5: MAP@7 scoring function ---\n",
    "def mapk(y_true_dict, y_pred_dict, k=7):\n",
    "    total_score = 0.0\n",
    "    for key, actual_list in y_true_dict.items():\n",
    "        pred_list = y_pred_dict.get(key, [])[:k]\n",
    "        score = 0.0\n",
    "        hits = 0\n",
    "        for i, p in enumerate(pred_list):\n",
    "            if p in actual_list:\n",
    "                hits += 1\n",
    "                score += hits / (i + 1)\n",
    "        if len(actual_list) == 0:\n",
    "            continue\n",
    "        total_score += score / min(len(actual_list), k)\n",
    "    return total_score / len(y_true_dict)\n",
    "\n",
    "# --- Step 6: Calculate MAP@7 ---\n",
    "score_ensemble = mapk(ground_truth, ensemble_preds)\n",
    "score_ranked = mapk(ground_truth, ranked_preds)\n",
    "score_pca = mapk(ground_truth, pca_preds)\n",
    "\n",
    "# --- Step 7: Print scores ---\n",
    "print(f\"✅ MAP@7 Ensemble: {score_ensemble:.6f}\")\n",
    "print(f\"✅ MAP@7 Ranked:   {score_ranked:.6f}\")\n",
    "print(f\"✅ MAP@7 PCA:      {score_pca:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "56abde36-f095-4741-ac72-872534272f0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"❌ 'id2' column not found in train_processed.csv.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# --- Step 2: Create ground truth dict grouped by id2 ---\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid2\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m train_df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid2\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column not found in train_processed.csv.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m train_df[train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid2\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlist\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# --- Step 3: Load prediction files ---\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"❌ 'id2' column not found in train_processed.csv.\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Load training data ---\n",
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "\n",
    "# --- Step 2: Create ground truth dict grouped by id2 ---\n",
    "if 'id2' not in train_df.columns:\n",
    "    raise KeyError(\"❌ 'id2' column not found in train_processed.csv.\")\n",
    "ground_truth = train_df[train_df['y'] == 1].groupby('id2')['id1'].apply(list).to_dict()\n",
    "\n",
    "# --- Step 3: Load prediction files ---\n",
    "ensemble_df = pd.read_csv(\"final_submission_ensemble.csv\")\n",
    "ranked_df = pd.read_csv(\"final_submission_rankedfinalfinal.csv\")\n",
    "pca_df = pd.read_csv(\"final_submission_ranked_pca.csv\")\n",
    "\n",
    "# --- Step 4: Convert predictions into grouped format using id2 ---\n",
    "ensemble_preds = ensemble_df.groupby('id2')['id1'].apply(list).to_dict()\n",
    "ranked_preds = ranked_df.groupby('id2')['id1'].apply(list).to_dict()\n",
    "pca_preds = pca_df.groupby('id2')['id1'].apply(list).to_dict()\n",
    "\n",
    "# --- Step 5: Define MAP@7 scoring function ---\n",
    "def mapk(y_true_dict, y_pred_dict, k=7):\n",
    "    total_score = 0.0\n",
    "    for key, actual in y_true_dict.items():\n",
    "        pred = y_pred_dict.get(key, [])[:k]\n",
    "        score = 0.0\n",
    "        hits = 0\n",
    "        for i, p in enumerate(pred):\n",
    "            if p in actual:\n",
    "                hits += 1\n",
    "                score += hits / (i + 1)\n",
    "        if len(actual) == 0:\n",
    "            continue\n",
    "        total_score += score / min(len(actual), k)\n",
    "    return total_score / len(y_true_dict)\n",
    "\n",
    "# --- Step 6: Calculate MAP@7 for each prediction file ---\n",
    "score_ensemble = mapk(ground_truth, ensemble_preds)\n",
    "score_ranked = mapk(ground_truth, ranked_preds)\n",
    "score_pca = mapk(ground_truth, pca_preds)\n",
    "\n",
    "# --- Step 7: Print results ---\n",
    "print(f\"✅ MAP@7 Ensemble: {score_ensemble:.6f}\")\n",
    "print(f\"✅ MAP@7 Ranked:   {score_ranked:.6f}\")\n",
    "print(f\"✅ MAP@7 PCA:      {score_pca:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9340b3c-4823-40e7-a627-4975f34a8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bd81946-97f4-46ec-afed-c2548d07b404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>offer_id</th>\n",
       "      <th>id4</th>\n",
       "      <th>id5</th>\n",
       "      <th>y</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>...</th>\n",
       "      <th>cashback_value</th>\n",
       "      <th>offer_duration_months</th>\n",
       "      <th>store_type_encoded</th>\n",
       "      <th>has_cashback</th>\n",
       "      <th>offer_ctr</th>\n",
       "      <th>industry_code</th>\n",
       "      <th>redemption_frequency</th>\n",
       "      <th>industry_total_spent</th>\n",
       "      <th>industry_avg_spent</th>\n",
       "      <th>industry_txn_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1366776_189706075_16-23_2023-11-02 22:22:00.042</td>\n",
       "      <td>1366776</td>\n",
       "      <td>189706075</td>\n",
       "      <td>2023-11-02 22:22:00.042</td>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.059875</td>\n",
       "      <td>57310000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16249283.79</td>\n",
       "      <td>433.638017</td>\n",
       "      <td>37472.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1366776_89227_16-23_2023-11-01 23:51:24.999</td>\n",
       "      <td>1366776</td>\n",
       "      <td>89227</td>\n",
       "      <td>2023-11-01 23:51:24.999</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046487</td>\n",
       "      <td>59210000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3511750.83</td>\n",
       "      <td>142.771510</td>\n",
       "      <td>24597.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1366776_35046_16-23_2023-11-01 00:30:59.797</td>\n",
       "      <td>1366776</td>\n",
       "      <td>35046</td>\n",
       "      <td>2023-11-01 00:30:59.797</td>\n",
       "      <td>2023-11-01</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.041484</td>\n",
       "      <td>72310000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2659689.37</td>\n",
       "      <td>137.033818</td>\n",
       "      <td>19409.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1366776_6275451_16-23_2023-11-02 22:21:32.261</td>\n",
       "      <td>1366776</td>\n",
       "      <td>6275451</td>\n",
       "      <td>2023-11-02 22:21:32.261</td>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042805</td>\n",
       "      <td>56510500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14216414.45</td>\n",
       "      <td>249.131054</td>\n",
       "      <td>57064.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1366776_78053_16-23_2023-11-02 22:21:34.799</td>\n",
       "      <td>1366776</td>\n",
       "      <td>78053</td>\n",
       "      <td>2023-11-02 22:21:34.799</td>\n",
       "      <td>2023-11-02</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.042544</td>\n",
       "      <td>59991300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2352086.84</td>\n",
       "      <td>167.946222</td>\n",
       "      <td>14005.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 382 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               id1  customer_id   offer_id  \\\n",
       "0  1366776_189706075_16-23_2023-11-02 22:22:00.042      1366776  189706075   \n",
       "1      1366776_89227_16-23_2023-11-01 23:51:24.999      1366776      89227   \n",
       "2      1366776_35046_16-23_2023-11-01 00:30:59.797      1366776      35046   \n",
       "3    1366776_6275451_16-23_2023-11-02 22:21:32.261      1366776    6275451   \n",
       "4      1366776_78053_16-23_2023-11-02 22:21:34.799      1366776      78053   \n",
       "\n",
       "                       id4         id5  y   f1  f2  f3  f4  ...  \\\n",
       "0  2023-11-02 22:22:00.042  2023-11-02  0  1.0 NaN NaN NaN  ...   \n",
       "1  2023-11-01 23:51:24.999  2023-11-01  0  1.0 NaN NaN NaN  ...   \n",
       "2  2023-11-01 00:30:59.797  2023-11-01  0  1.0 NaN NaN NaN  ...   \n",
       "3  2023-11-02 22:21:32.261  2023-11-02  0  1.0 NaN NaN NaN  ...   \n",
       "4  2023-11-02 22:21:34.799  2023-11-02  0  1.0 NaN NaN NaN  ...   \n",
       "\n",
       "   cashback_value  offer_duration_months  store_type_encoded  has_cashback  \\\n",
       "0             2.0                    1.0               104.0           1.0   \n",
       "1             0.0                    6.0                68.0           0.0   \n",
       "2            10.0                    1.0                13.0           1.0   \n",
       "3            10.0                    1.0                47.0           1.0   \n",
       "4             8.0                    1.0                86.0           1.0   \n",
       "\n",
       "   offer_ctr  industry_code  redemption_frequency  industry_total_spent  \\\n",
       "0   0.059875     57310000.0                   2.0           16249283.79   \n",
       "1   0.046487     59210000.0                   2.0            3511750.83   \n",
       "2   0.041484     72310000.0                   2.0            2659689.37   \n",
       "3   0.042805     56510500.0                   2.0           14216414.45   \n",
       "4   0.042544     59991300.0                   2.0            2352086.84   \n",
       "\n",
       "   industry_avg_spent  industry_txn_count  \n",
       "0          433.638017             37472.0  \n",
       "1          142.771510             24597.0  \n",
       "2          137.033818             19409.0  \n",
       "3          249.131054             57064.0  \n",
       "4          167.946222             14005.0  \n",
       "\n",
       "[5 rows x 382 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a9f75e0f-66be-49b2-882e-658a57fcbdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MAP@7 Ensemble: 0.000000\n",
      "✅ MAP@7 Ranked:   0.000000\n",
      "✅ MAP@7 PCA:      0.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Load training data and rename column ---\n",
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "train_df = train_df.rename(columns={\"customer_id\": \"id2\"})\n",
    "\n",
    "# --- Step 2: Create ground truth dict grouped by id2 ---\n",
    "ground_truth = train_df[train_df['y'] == 1].groupby('id2')['id1'].apply(list).to_dict()\n",
    "\n",
    "# --- Step 3: Load prediction files ---\n",
    "ensemble_df = pd.read_csv(\"final_submission_ensemble.csv\")\n",
    "ranked_df = pd.read_csv(\"final_submission_rankedfinalfinal.csv\")\n",
    "pca_df = pd.read_csv(\"final_submission_ranked_pca.csv\")\n",
    "\n",
    "# --- Step 4: Convert predictions into grouped format using id2 ---\n",
    "ensemble_preds = ensemble_df.groupby('id2')['id1'].apply(list).to_dict()\n",
    "ranked_preds = ranked_df.groupby('id2')['id1'].apply(list).to_dict()\n",
    "pca_preds = pca_df.groupby('id2')['id1'].apply(list).to_dict()\n",
    "\n",
    "# --- Step 5: Define MAP@7 scoring function ---\n",
    "def mapk(y_true_dict, y_pred_dict, k=7):\n",
    "    total_score = 0.0\n",
    "    for key, actual in y_true_dict.items():\n",
    "        pred = y_pred_dict.get(key, [])[:k]\n",
    "        score = 0.0\n",
    "        hits = 0\n",
    "        for i, p in enumerate(pred):\n",
    "            if p in actual:\n",
    "                hits += 1\n",
    "                score += hits / (i + 1)\n",
    "        if len(actual) == 0:\n",
    "            continue\n",
    "        total_score += score / min(len(actual), k)\n",
    "    return total_score / len(y_true_dict)\n",
    "\n",
    "# --- Step 6: Calculate MAP@7 for each prediction file ---\n",
    "score_ensemble = mapk(ground_truth, ensemble_preds)\n",
    "score_ranked = mapk(ground_truth, ranked_preds)\n",
    "score_pca = mapk(ground_truth, pca_preds)\n",
    "\n",
    "# --- Step 7: Print results ---\n",
    "print(f\"✅ MAP@7 Ensemble: {score_ensemble:.6f}\")\n",
    "print(f\"✅ MAP@7 Ranked:   {score_ranked:.6f}\")\n",
    "print(f\"✅ MAP@7 PCA:      {score_pca:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a65f1-b0e6-4ba9-b125-85bad433e3b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db961641-3a83-4ef6-9af8-1999b4c7a543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 29641, number of negative: 586490\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.174545 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 43197\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 268\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.048108 -> initscore=-2.984997\n",
      "[LightGBM] [Info] Start training from score -2.984997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahee\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [19:43:41] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# --- Helper function to compute MAP@7 ---\n",
    "def apk(actual, predicted, k=7):\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(actual), k) if actual else 0.0\n",
    "\n",
    "def mapk(actual_dict, predicted_dict, k=7):\n",
    "    scores = [apk(actual_dict.get(key, []), predicted_dict.get(key, []), k) for key in predicted_dict.keys()]\n",
    "    return np.mean(scores)\n",
    "\n",
    "# --- Step 1: Load data ---\n",
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "\n",
    "# Rename customer_id to id2\n",
    "train_df.rename(columns={'customer_id': 'id2'}, inplace=True)\n",
    "\n",
    "# --- Step 2: Encode categorical features ---\n",
    "categorical_cols = ['f42', 'f50', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f354']\n",
    "le_dict = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    train_df[col] = le.fit_transform(train_df[col].astype(str))\n",
    "    le_dict[col] = le\n",
    "\n",
    "# --- Step 3: Drop columns with >60% missing values ---\n",
    "threshold = 0.6\n",
    "missing_ratio = train_df.isnull().mean()\n",
    "cols_to_drop = missing_ratio[missing_ratio > threshold].index\n",
    "train_df = train_df.drop(columns=cols_to_drop)\n",
    "\n",
    "# --- Step 4: Fill missing values with mean ---\n",
    "train_df = train_df.fillna(train_df.mean(numeric_only=True))\n",
    "\n",
    "# --- Step 5: Prepare features and target ---\n",
    "X = train_df.drop(columns=['id1', 'offer_id', 'y', 'id4', 'id5'])\n",
    "y = train_df['y']\n",
    "\n",
    "# --- Step 6: Train/Validation Split ---\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Restore ids for validation set\n",
    "y_val_df = train_df.loc[X_val.index, ['id1', 'id2', 'y']].copy()\n",
    "\n",
    "# --- Step 7: Train Models ---\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=300, learning_rate=0.05, num_leaves=31)\n",
    "lgb_model.fit(X_train, y_train)\n",
    "val_preds_lgb = lgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(n_estimators=300, learning_rate=0.05, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "val_preds_xgb = xgb_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "val_preds_rf = rf_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# --- Step 8: Ensemble Predictions on Validation ---\n",
    "y_val_df['raw_pred'] = 0.4 * val_preds_lgb + 0.3 * val_preds_xgb + 0.3 * val_preds_rf\n",
    "\n",
    "# --- Step 9: Rank within id2 ---\n",
    "y_val_df['rank'] = y_val_df.groupby('id2')['raw_pred'].rank(method='first', ascending=False)\n",
    "y_val_df['inv_rank'] = y_val_df.groupby('id2')['rank'].transform(lambda r: 1 / r)\n",
    "y_val_df['pred'] = y_val_df.groupby('id2')['inv_rank'].transform(lambda x: x / x.sum())\n",
    "\n",
    "# --- Step 10: Evaluate MAP@7 ---\n",
    "actual_dict = y_val_df[y_val_df['y'] == 1].groupby('id2')['id1'].apply(list).to_dict()\n",
    "pred_dict = y_val_df.sort_values(by='pred', ascending=False).groupby('id2')['id1'].apply(list).to_dict()\n",
    "\n",
    "score = mapk(actual_dict, pred_dict, k=7)\n",
    "print(f\"\\n✅ MAP@7 Ensemble on Validation Set: {score:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168712a-bf8a-48ac-bac1-94292eec158d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a140d4e1-0fb9-4680-a325-9714e00495b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "516d0b05-1576-4a86-b766-76fce87ef15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function to compute MAP@7 ---\n",
    "def apk(actual, predicted, k=7):\n",
    "    if len(predicted) > k:\n",
    "        predicted = predicted[:k]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(actual), k) if actual else 0.0\n",
    "\n",
    "def mapk(actual_dict, predicted_dict, k=7):\n",
    "    scores = [apk(actual_dict.get(key, []), predicted_dict.get(key, []), k) for key in predicted_dict.keys()]\n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c28100-b718-4ef5-81c9-34e54aa2d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_processed.csv\")\n",
    "test_df = pd.read_csv(\"test_processed.csv\")\n",
    "\n",
    "# Rename customer_id to id2\n",
    "train_df.rename(columns={'customer_id': 'id2'}, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e254a008-3910-4ee8-93e1-83cb45dfd509",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['f42', 'f50', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f354']\n",
    "le_dict = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([train_df[col], test_df[col]], axis=0).astype(str)\n",
    "    le.fit(combined)\n",
    "    train_df[col] = le.transform(train_df[col].astype(str))\n",
    "    test_df[col] = le.transform(test_df[col].astype(str))\n",
    "    le_dict[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fe5a6d2-ee8a-4d79-a4c5-ae0efbcf4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.drop(columns=['id1', 'offer_id', 'y', 'id4', 'id5'])\n",
    "y = train_df['y']\n",
    "X_test = test_df.drop(columns=['id1', 'offer_id', 'id4', 'id5', 'y'], errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dc2c89-271b-4a28-8806-7868de9b6259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "[LightGBM] [Info] Number of positive: 37051, number of negative: 733113\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.454965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 49126\n",
      "[LightGBM] [Info] Number of data points in the train set: 770164, number of used features: 373\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.048108 -> initscore=-2.985005\n",
      "[LightGBM] [Info] Start training from score -2.985005\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- LightGBM ---\n",
    "lgb_params = {\n",
    "    'n_estimators': [300],\n",
    "    'learning_rate': [0.05],\n",
    "    'num_leaves': [31, 63],\n",
    "}\n",
    "lgb_model = GridSearchCV(lgb.LGBMClassifier(), lgb_params, cv=skf, scoring='neg_log_loss', verbose=1, n_jobs=-1)\n",
    "lgb_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46423992-cc8a-4223-9e29-5ed1a7ccaffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'n_estimators': [300],\n",
    "    'learning_rate': [0.05],\n",
    "    'max_depth': [3, 6]\n",
    "}\n",
    "xgb_model = GridSearchCV(xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgb_params, cv=skf, scoring='neg_log_loss', verbose=1, n_jobs=-1)\n",
    "xgb_model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d2026-a25f-48d7-9f2d-60f554e95e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators': [200],\n",
    "    'max_depth': [None, 10]\n",
    "}\n",
    "rf_model = GridSearchCV(RandomForestClassifier(random_state=42), rf_params, cv=skf, scoring='neg_log_loss', verbose=1, n_jobs=-1)\n",
    "rf_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df4669-e260-4d1c-939b-8acde6bb2b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_lgb = lgb_model.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "probs_xgb = xgb_model.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "probs_rf = rf_model.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_df['raw_pred'] = 0.4 * probs_lgb + 0.3 * probs_xgb + 0.3 * probs_rf\n",
    "\n",
    "# --- Step 8: Rank and Normalize ---\n",
    "test_df['rank'] = test_df.groupby('id2')['raw_pred'].rank(method='first', ascending=False)\n",
    "test_df['inv_rank'] = test_df.groupby('id2')['rank'].transform(lambda r: 1 / r)\n",
    "test_df['pred'] = test_df.groupby('id2')['inv_rank'].transform(lambda x: x / x.sum())\n",
    "\n",
    "# --- Step 9: Save final submission ---\n",
    "submission_df = test_df[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "submission_df.to_csv(\"final_submission_ensemble_full.csv\", index=False)\n",
    "print(\"✅ Final submission saved as 'final_submission_ensemble_full.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7e008e-2f9b-4b59-b7b9-b8bdf92e576f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c6aa650-96ea-4060-81a0-ca0001ec73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===================================================================================\n",
    "# 0. Memory Reduction Helper\n",
    "# ===================================================================================\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min, c_max = df[col].min(), df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print(f'Mem. usage decreased to {end_mem:5.2f} Mb ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
    "    return df\n",
    "\n",
    "# ===================================================================================\n",
    "# 1. Custom MAP@7 Metric\n",
    "# ===================================================================================\n",
    "def mapk(actuals, predicteds, k=7):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actuals, predicteds)])\n",
    "\n",
    "def apk(actual, predicted, k=7):\n",
    "    if len(predicted) > k: predicted = predicted[:k]\n",
    "    score, num_hits = 0.0, 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    if not actual: return 0.0\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# ===================================================================================\n",
    "# 2. Data Loading and Cleaning (Robust Version)\n",
    "# ===================================================================================\n",
    "def load_and_clean_data(data_path='.'):\n",
    "    print(\"Loading and cleaning data...\")\n",
    "    train = pd.read_parquet(f'{data_path}/train_data.parquet')\n",
    "    test = pd.read_parquet(f'{data_path}/test_data.parquet')\n",
    "    offer_meta = pd.read_parquet(f'{data_path}/offer_metadata.parquet')\n",
    "    events = pd.read_parquet(f'{data_path}/add_event.parquet')\n",
    "    trans = pd.read_parquet(f'{data_path}/add_trans.parquet')\n",
    "\n",
    "    # === ROBUST FIX for ValueError ===\n",
    "    # Force all feature columns ('f*') to numeric types robustly in both train and test.\n",
    "    for df in [train, test]:\n",
    "        for col in df.columns:\n",
    "            if col.startswith('f'):\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df['id5'] = pd.to_datetime(df['id5'], errors='coerce')\n",
    "\n",
    "    # Ensure all merge keys are the same numeric type\n",
    "    for df in [train, test, offer_meta, events, trans]:\n",
    "        for col in ['id2', 'id3', 'customer_id', 'offer_id']:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    offer_meta = offer_meta.rename(columns={'id3': 'offer_id', 'id9': 'industry_code', 'id12': 'start_date', 'id13': 'end_date'})\n",
    "    events = events.rename(columns={'id2': 'customer_id', 'id3': 'offer_id', 'id4': 'impression_timestamp', 'id7': 'click_timestamp'})\n",
    "    trans = trans.rename(columns={'id2': 'customer_id', 'f367': 'transaction_amount', 'f370': 'transaction_date_d', 'f371': 'transaction_date_t'})\n",
    "\n",
    "    # Convert date columns\n",
    "    trans['transaction_date'] = pd.to_datetime(trans['transaction_date_d'] + ' ' + trans['transaction_date_t'], errors='coerce')\n",
    "    for df in [offer_meta, events]:\n",
    "        for col in df.columns:\n",
    "            if 'date' in col or 'timestamp' in col:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    return train, test, offer_meta, events, trans\n",
    "\n",
    "# ===================================================================================\n",
    "# 3. Memory-Efficient Feature Engineering\n",
    "# ===================================================================================\n",
    "def feature_engineer(df, offer_meta, events, trans):\n",
    "    print(f\"Engineering features for {'train' if 'y' in df.columns else 'test'} set...\")\n",
    "    \n",
    "    # Merge offer metadata\n",
    "    df = df.merge(offer_meta, left_on='id3', right_on='offer_id', how='left')\n",
    "    df['imp_date'] = pd.to_datetime(df['id5'])\n",
    "    \n",
    "    # Time-based Features\n",
    "    df['imp_dow'] = df['imp_date'].dt.dayofweek\n",
    "    df['offer_duration'] = (df['end_date'] - df['start_date']).dt.days\n",
    "\n",
    "    # Customer Historical Behavior\n",
    "    events['has_clicked'] = events['click_timestamp'].notna().astype(int)\n",
    "    cust_agg = events.groupby('customer_id').agg(\n",
    "        total_customer_imps=('impression_timestamp', 'count'),\n",
    "        total_customer_clicks=('has_clicked', 'sum'),\n",
    "        last_click_date=('click_timestamp', 'max')\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "    df['days_since_last_any_click'] = (df['imp_date'] - df['last_click_date']).dt.days\n",
    "\n",
    "    # Customer-Industry Interaction\n",
    "    cust_industry_agg = events.merge(offer_meta[['offer_id', 'industry_code']], on='offer_id')\n",
    "    cust_industry_agg = cust_industry_agg.groupby(['customer_id', 'industry_code']).agg(\n",
    "        customer_industry_imps=('impression_timestamp', 'count'),\n",
    "        customer_industry_clicks=('has_clicked', 'sum')\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_industry_agg, left_on=['id2', 'industry_code'], right_on=['customer_id', 'industry_code'], how='left')\n",
    "\n",
    "    # Transaction-based Features\n",
    "    trans_agg = trans.groupby('customer_id').agg(\n",
    "        avg_spend=('transaction_amount', 'mean'),\n",
    "        last_trans_date=('transaction_date', 'max')\n",
    "    ).reset_index()\n",
    "    df = df.merge(trans_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ===================================================================================\n",
    "# 4. Model Training\n",
    "# ===================================================================================\n",
    "def train_model(X, y, groups, X_test):\n",
    "    print(\"Starting model training...\")\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    oof = np.zeros(len(X))\n",
    "    test_pred = np.zeros(len(X_test))\n",
    "\n",
    "    for fold, (t_idx, v_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "        print(f\"--- Fold {fold+1}/5 ---\")\n",
    "        Xt, Xv = X.iloc[t_idx], X.iloc[v_idx]\n",
    "        yt, yv = y.iloc[t_idx], y.iloc[v_idx]\n",
    "\n",
    "        sort_idx_t = Xt['id2'].argsort()\n",
    "        Xt, yt = Xt.iloc[sort_idx_t], yt.iloc[sort_idx_t]\n",
    "        sort_idx_v = Xv['id2'].argsort()\n",
    "        Xv, yv = Xv.iloc[sort_idx_v], yv.iloc[sort_idx_v]\n",
    "\n",
    "        gr_t, gr_v = Xt.groupby('id2').size().to_numpy(), Xv.groupby('id2').size().to_numpy()\n",
    "        \n",
    "        Xt_, Xv_ = Xt.drop('id2', axis=1), Xv.drop('id2', axis=1)\n",
    "        Xtest_ = X_test.drop('id2', axis=1)\n",
    "\n",
    "        model_lgb = lgb.LGBMRanker(objective='lambdarank', metric='map', eval_at=7, n_estimators=2000, learning_rate=0.03, num_leaves=80, random_state=42+fold)\n",
    "        model_lgb.fit(Xt_, yt, group=gr_t, eval_set=[(Xv_, yv)], eval_group=[gr_v], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "        \n",
    "        p1 = model_lgb.predict(Xv_)\n",
    "        tp1 = model_lgb.predict(Xtest_)\n",
    "        \n",
    "        def rank(x): return pd.Series(x).rank(method='first').values\n",
    "        \n",
    "        oof_preds_fold = pd.Series(rank(p1), index=Xv.index)\n",
    "        oof[v_idx] = oof_preds_fold.reindex(Xt.iloc[v_idx].index).values\n",
    "        test_pred += rank(tp1)\n",
    "        \n",
    "        del model_lgb; gc.collect()\n",
    "\n",
    "    test_pred /= gkf.get_n_splits()\n",
    "    print(\"Model training complete.\")\n",
    "    return oof, test_pred\n",
    "\n",
    "# ===================================================================================\n",
    "# 5. Main Execution and Submission\n",
    "# ===================================================================================\n",
    "if __name__ == \"_main_\":\n",
    "    data_path = '.'\n",
    "    train_df, test_df, offer_meta_df, events_df, trans_df = load_and_clean_data(data_path)\n",
    "    \n",
    "    train_proc = feature_engineer(train_df, offer_meta_df, events_df, trans_df)\n",
    "    test_proc = feature_engineer(test_df, offer_meta_df, events_df, trans_df)\n",
    "    \n",
    "    del train_df, test_df, offer_meta_df, events_df, trans_df; gc.collect()\n",
    "\n",
    "    # Define feature columns\n",
    "    original_features = [c for c in train_proc.columns if c.startswith('f')]\n",
    "    new_features = ['imp_dow', 'offer_duration', 'total_customer_imps', 'total_customer_clicks', 'days_since_last_any_click', 'customer_industry_imps', 'customer_industry_clicks', 'avg_spend']\n",
    "    feature_cols = original_features + new_features\n",
    "\n",
    "    # Align columns and handle missing values\n",
    "    test_proc = test_proc.reindex(columns=train_proc.columns, fill_value=0)\n",
    "    train_proc.fillna(0, inplace=True)\n",
    "    test_proc.fillna(0, inplace=True)\n",
    "\n",
    "    X, y, grp = train_proc[feature_cols + ['id2']], train_proc['y'], train_proc['id2']\n",
    "    X_test = test_proc[feature_cols + ['id2']]\n",
    "\n",
    "    oof_preds, test_preds = train_model(X, y, grp, X_test)\n",
    "\n",
    "    # Evaluation\n",
    "    train_proc['pred'] = oof_preds\n",
    "    oof_ranked = train_proc.sort_values(['id2','pred'], ascending=[True,False]).groupby('id2')['id3'].apply(list).reset_index(name='predicted_offers')\n",
    "    true_offers = train_proc[train_proc['y']==1].groupby('id2')['id3'].apply(list).reset_index(name='true_offers')\n",
    "    eval_df = oof_ranked.merge(true_offers, on='id2', how='left')\n",
    "    eval_df['true_offers'] = eval_df['true_offers'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    final_map7 = mapk(eval_df['true_offers'], eval_df['predicted_offers'])\n",
    "    print(f\"\\nFinal Estimated OOF MAP@7: {final_map7:.6f}\")\n",
    "\n",
    "    # Submission\n",
    "    print(\"Creating submission file...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    test_proc['pred'] = scaler.fit_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    submission_df = test_proc[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "    submission_df['id5'] = pd.to_datetime(submission_df['id5']).dt.strftime('%m/%d/%Y')\n",
    "\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    print(\"Submission saved to 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106078c8-2c33-4ab8-822a-eca68608a6a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154f8ca7-bf8d-49a8-87a0-a987761e707d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "Engineering features for train set...\n",
      "Engineering features for train set...\n",
      "Starting model training...\n",
      "--- Fold 1/5 ---\n",
      "[LightGBM] [Info] Total groups: 37240, total data: 616131\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.273159 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47330\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 296\n",
      "[LightGBM] [Info] Total groups: 9310, total data: 154033\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's map@7: 0.932995\n",
      "--- Fold 2/5 ---\n",
      "[LightGBM] [Info] Total groups: 37240, total data: 616131\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.323433 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47337\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 297\n",
      "[LightGBM] [Info] Total groups: 9310, total data: 154033\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[356]\tvalid_0's map@7: 0.931429\n",
      "--- Fold 3/5 ---\n",
      "[LightGBM] [Info] Total groups: 37240, total data: 616131\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.365540 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47283\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 296\n",
      "[LightGBM] [Info] Total groups: 9310, total data: 154033\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[262]\tvalid_0's map@7: 0.936099\n",
      "--- Fold 4/5 ---\n",
      "[LightGBM] [Info] Total groups: 37240, total data: 616131\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.389642 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47318\n",
      "[LightGBM] [Info] Number of data points in the train set: 616131, number of used features: 296\n",
      "[LightGBM] [Info] Total groups: 9310, total data: 154033\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[234]\tvalid_0's map@7: 0.936939\n",
      "--- Fold 5/5 ---\n",
      "[LightGBM] [Info] Total groups: 37240, total data: 616132\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.442560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 47395\n",
      "[LightGBM] [Info] Number of data points in the train set: 616132, number of used features: 297\n",
      "[LightGBM] [Info] Total groups: 9310, total data: 154032\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[311]\tvalid_0's map@7: 0.938011\n",
      "\n",
      "Final Estimated OOF MAP@7: 0.049591\n",
      "\n",
      "✅ Submission saved to 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===================================================================================\n",
    "# 0. Memory Reduction Helper\n",
    "# ===================================================================================\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min, c_max = df[col].min(), df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print(f'Mem. usage decreased to {end_mem:5.2f} Mb ({100 * (start_mem - end_mem) / start_mem:.1f}% reduction)')\n",
    "    return df\n",
    "\n",
    "# ===================================================================================\n",
    "# 1. Custom MAP@7 Metric\n",
    "# ===================================================================================\n",
    "def mapk(actuals, predicteds, k=7):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actuals, predicteds)])\n",
    "\n",
    "def apk(actual, predicted, k=7):\n",
    "    if len(predicted) > k: predicted = predicted[:k]\n",
    "    score, num_hits = 0.0, 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    if not actual: return 0.0\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# ===================================================================================\n",
    "# 2. Data Loading and Cleaning (Robust Version)\n",
    "# ===================================================================================\n",
    "def load_and_clean_data(data_path='.'):\n",
    "    print(\"Loading and cleaning data...\")\n",
    "    train = pd.read_parquet(f'{data_path}/train_data.parquet')\n",
    "    test = pd.read_parquet(f'{data_path}/test_data.parquet')\n",
    "    offer_meta = pd.read_parquet(f'{data_path}/offer_metadata.parquet')\n",
    "    events = pd.read_parquet(f'{data_path}/add_event.parquet')\n",
    "    trans = pd.read_parquet(f'{data_path}/add_trans.parquet')\n",
    "\n",
    "    for df in [train, test]:\n",
    "        for col in df.columns:\n",
    "            if col.startswith('f'):\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df['id5'] = pd.to_datetime(df['id5'], errors='coerce')\n",
    "        df['y'] = pd.to_numeric(df.get('y', 0), errors='coerce')\n",
    "\n",
    "    for df in [train, test, offer_meta, events, trans]:\n",
    "        for col in ['id2', 'id3', 'customer_id', 'offer_id']:\n",
    "            if col in df.columns:\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    offer_meta = offer_meta.rename(columns={'id3': 'offer_id', 'id9': 'industry_code', 'id12': 'start_date', 'id13': 'end_date'})\n",
    "    events = events.rename(columns={'id2': 'customer_id', 'id3': 'offer_id', 'id4': 'impression_timestamp', 'id7': 'click_timestamp'})\n",
    "    trans = trans.rename(columns={'id2': 'customer_id', 'f367': 'transaction_amount', 'f370': 'transaction_date_d', 'f371': 'transaction_date_t'})\n",
    "\n",
    "    trans['transaction_date'] = pd.to_datetime(trans['transaction_date_d'] + ' ' + trans['transaction_date_t'], errors='coerce')\n",
    "    for df in [offer_meta, events]:\n",
    "        for col in df.columns:\n",
    "            if 'date' in col or 'timestamp' in col:\n",
    "                df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    return train, test, offer_meta, events, trans\n",
    "\n",
    "# ===================================================================================\n",
    "# 3. Feature Engineering\n",
    "# ===================================================================================\n",
    "def feature_engineer(df, offer_meta, events, trans):\n",
    "    print(f\"Engineering features for {'train' if 'y' in df.columns else 'test'} set...\")\n",
    "    df = df.merge(offer_meta, left_on='id3', right_on='offer_id', how='left')\n",
    "    df['imp_date'] = pd.to_datetime(df['id5'])\n",
    "    df['imp_dow'] = df['imp_date'].dt.dayofweek\n",
    "    df['offer_duration'] = (df['end_date'] - df['start_date']).dt.days\n",
    "\n",
    "    events['has_clicked'] = events['click_timestamp'].notna().astype(int)\n",
    "    cust_agg = events.groupby('customer_id').agg(\n",
    "        total_customer_imps=('impression_timestamp', 'count'),\n",
    "        total_customer_clicks=('has_clicked', 'sum'),\n",
    "        last_click_date=('click_timestamp', 'max')\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "    df['days_since_last_any_click'] = (df['imp_date'] - df['last_click_date']).dt.days\n",
    "\n",
    "    cust_industry_agg = events.merge(offer_meta[['offer_id', 'industry_code']], on='offer_id')\n",
    "    cust_industry_agg = cust_industry_agg.groupby(['customer_id', 'industry_code']).agg(\n",
    "        customer_industry_imps=('impression_timestamp', 'count'),\n",
    "        customer_industry_clicks=('has_clicked', 'sum')\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_industry_agg, left_on=['id2', 'industry_code'], right_on=['customer_id', 'industry_code'], how='left')\n",
    "\n",
    "    trans_agg = trans.groupby('customer_id').agg(\n",
    "        avg_spend=('transaction_amount', 'mean'),\n",
    "        last_trans_date=('transaction_date', 'max')\n",
    "    ).reset_index()\n",
    "    df = df.merge(trans_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "# ===================================================================================\n",
    "# 4. Model Training\n",
    "# ===================================================================================\n",
    "def train_model(X, y, groups, X_test):\n",
    "    print(\"Starting model training...\")\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    oof = np.zeros(len(X))\n",
    "    test_pred = np.zeros(len(X_test))\n",
    "\n",
    "    for fold, (t_idx, v_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "        print(f\"--- Fold {fold+1}/5 ---\")\n",
    "        Xt, Xv = X.iloc[t_idx], X.iloc[v_idx]\n",
    "        yt, yv = y.iloc[t_idx].astype(float), y.iloc[v_idx].astype(float)\n",
    "\n",
    "        gr_t = Xt['id2'].value_counts().sort_index().values\n",
    "        gr_v = Xv['id2'].value_counts().sort_index().values\n",
    "        Xt_, Xv_ = Xt.drop('id2', axis=1), Xv.drop('id2', axis=1)\n",
    "        Xtest_ = X_test.drop('id2', axis=1)\n",
    "\n",
    "        model = lgb.LGBMRanker(objective='lambdarank', metric='map', eval_at=7, learning_rate=0.03, n_estimators=2000, num_leaves=80, random_state=fold+42)\n",
    "        model.fit(Xt_, yt, group=gr_t, eval_set=[(Xv_, yv)], eval_group=[gr_v], callbacks=[lgb.early_stopping(100)])\n",
    "\n",
    "        pred_val = model.predict(Xv_)\n",
    "        pred_test = model.predict(Xtest_)\n",
    "\n",
    "        oof[v_idx] = pd.Series(pred_val).rank(method='first').values\n",
    "        test_pred += pd.Series(pred_test).rank(method='first').values\n",
    "        del model; gc.collect()\n",
    "\n",
    "    test_pred /= gkf.get_n_splits()\n",
    "    return oof, test_pred\n",
    "\n",
    "# ===================================================================================\n",
    "# 5. Main Execution Block\n",
    "# ===================================================================================\n",
    "if __name__ == '__main__':\n",
    "    data_path = '.'\n",
    "    train_df, test_df, offer_meta, events, trans = load_and_clean_data(data_path)\n",
    "\n",
    "    train_df = feature_engineer(train_df, offer_meta, events, trans)\n",
    "    test_df = feature_engineer(test_df, offer_meta, events, trans)\n",
    "\n",
    "    feature_cols = [c for c in train_df.columns if c.startswith('f') and train_df[c].dtype in [np.float32, np.float64, np.int32, np.int64]] + [\n",
    "        'imp_dow', 'offer_duration', 'total_customer_imps', 'total_customer_clicks',\n",
    "        'days_since_last_any_click', 'customer_industry_imps', 'customer_industry_clicks', 'avg_spend']\n",
    "\n",
    "    train_df.fillna(0, inplace=True)\n",
    "    test_df.fillna(0, inplace=True)\n",
    "\n",
    "    X = train_df[feature_cols + ['id2']]\n",
    "    y = pd.to_numeric(train_df['y'], errors='coerce').fillna(0).astype(float)\n",
    "    X_test = test_df[feature_cols + ['id2']]\n",
    "\n",
    "    oof, test_preds = train_model(X, y, X['id2'], X_test)\n",
    "\n",
    "    train_df['pred'] = oof\n",
    "    oof_ranked = train_df.sort_values(['id2','pred'], ascending=[True,False]).groupby('id2')['id3'].apply(list).reset_index(name='predicted_offers')\n",
    "    true_offers = train_df[train_df['y']==1].groupby('id2')['id3'].apply(list).reset_index(name='true_offers')\n",
    "    eval_df = oof_ranked.merge(true_offers, on='id2', how='left')\n",
    "    eval_df['true_offers'] = eval_df['true_offers'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "    final_map7 = mapk(eval_df['true_offers'], eval_df['predicted_offers'])\n",
    "    print(f\"\\nFinal Estimated OOF MAP@7: {final_map7:.6f}\")\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    test_df['pred'] = scaler.fit_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "    submission = test_df[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "    submission['id5'] = pd.to_datetime(submission['id5']).dt.strftime('%m/%d/%Y')\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"\\n✅ Submission saved to 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6403b59-a746-4a36-8f1d-6b8faf6bff6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MinMinScaler' from 'sklearn.preprocessing' (C:\\Users\\fahee\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GroupKFold\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MinMinScaler\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomizedSearchCV\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBRanker\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'MinMinScaler' from 'sklearn.preprocessing' (C:\\Users\\fahee\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import MinMinScaler\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRanker\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===================================================================================\n",
    "# 1. Custom MAP@7 Metric\n",
    "# ===================================================================================\n",
    "def mapk(actuals, predicteds, k=7):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actuals, predicteds)])\n",
    "\n",
    "def apk(actual, predicted, k=7):\n",
    "    if len(predicted) > k: predicted = predicted[:k]\n",
    "    score, num_hits = 0.0, 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    if not actual: return 0.0\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# ===================================================================================\n",
    "# 2. Load and Prepare Data\n",
    "# ===================================================================================\n",
    "train = pd.read_parquet('train_data.parquet')\n",
    "test = pd.read_parquet('test_data.parquet')\n",
    "offer_meta = pd.read_parquet('offer_metadata.parquet')\n",
    "events = pd.read_parquet('add_event.parquet')\n",
    "trans = pd.read_parquet('add_trans.parquet')\n",
    "\n",
    "for df in [train, test]:\n",
    "    for col in df.columns:\n",
    "        if col.startswith('f'):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df['id5'] = pd.to_datetime(df['id5'], errors='coerce')\n",
    "    df['y'] = pd.to_numeric(df.get('y', 0), errors='coerce')\n",
    "\n",
    "for df in [train, test, offer_meta, events, trans]:\n",
    "    for col in ['id2', 'id3', 'customer_id', 'offer_id']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "offer_meta.rename(columns={'id3': 'offer_id', 'id9': 'industry_code', 'id12': 'start_date', 'id13': 'end_date'}, inplace=True)\n",
    "events.rename(columns={'id2': 'customer_id', 'id3': 'offer_id', 'id4': 'impression_timestamp', 'id7': 'click_timestamp'}, inplace=True)\n",
    "trans.rename(columns={'id2': 'customer_id', 'f367': 'transaction_amount', 'f370': 'transaction_date_d', 'f371': 'transaction_date_t'}, inplace=True)\n",
    "trans['transaction_date'] = pd.to_datetime(trans['transaction_date_d'] + ' ' + trans['transaction_date_t'], errors='coerce')\n",
    "\n",
    "# ===================================================================================\n",
    "# 3. Feature Engineering\n",
    "# ===================================================================================\n",
    "def feature_engineer(df):\n",
    "    df = df.merge(offer_meta, left_on='id3', right_on='offer_id', how='left')\n",
    "    df['imp_date'] = pd.to_datetime(df['id5'])\n",
    "    df['imp_dow'] = df['imp_date'].dt.dayofweek\n",
    "    df['offer_duration'] = (df['end_date'] - df['start_date']).dt.days\n",
    "\n",
    "    events['has_clicked'] = events['click_timestamp'].notna().astype(int)\n",
    "    cust_agg = events.groupby('customer_id').agg(\n",
    "        total_customer_imps=('impression_timestamp', 'count'),\n",
    "        total_customer_clicks=('has_clicked', 'sum'),\n",
    "        last_click_date=('click_timestamp', 'max')\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "    df['days_since_last_any_click'] = (df['imp_date'] - df['last_click_date']).dt.days\n",
    "\n",
    "    cust_industry_agg = events.merge(offer_meta[['offer_id', 'industry_code']], on='offer_id')\n",
    "    cust_industry_agg = cust_industry_agg.groupby(['customer_id', 'industry_code']).agg(\n",
    "        customer_industry_imps=('impression_timestamp', 'count'),\n",
    "        customer_industry_clicks=('has_clicked', 'sum')\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_industry_agg, left_on=['id2', 'industry_code'], right_on=['customer_id', 'industry_code'], how='left')\n",
    "\n",
    "    trans_agg = trans.groupby('customer_id').agg(\n",
    "        avg_spend=('transaction_amount', 'mean'),\n",
    "        last_trans_date=('transaction_date', 'max')\n",
    "    ).reset_index()\n",
    "    df = df.merge(trans_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = feature_engineer(train)\n",
    "test = feature_engineer(test)\n",
    "\n",
    "feature_cols = [c for c in train.columns if c.startswith('f') and train[c].dtype in [np.float32, np.float64, np.int32, np.int64]] + [\n",
    "    'imp_dow', 'offer_duration', 'total_customer_imps', 'total_customer_clicks',\n",
    "    'days_since_last_any_click', 'customer_industry_imps', 'customer_industry_clicks', 'avg_spend']\n",
    "\n",
    "train.fillna(0, inplace=True)\n",
    "test.fillna(0, inplace=True)\n",
    "\n",
    "X = train[feature_cols + ['id2']]\n",
    "y = pd.to_numeric(train['y'], errors='coerce').fillna(0).astype(float)\n",
    "X_test = test[feature_cols + ['id2']]\n",
    "\n",
    "# ===================================================================================\n",
    "# 4. Ensemble Training\n",
    "# ===================================================================================\n",
    "def train_ensemble(X, y, groups, X_test):\n",
    "    print(\"Training ensemble model (LGB + XGB)...\")\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    oof_lgb, oof_xgb = np.zeros(len(X)), np.zeros(len(X))\n",
    "    test_lgb, test_xgb = np.zeros(len(X_test)), np.zeros(len(X_test))\n",
    "\n",
    "    for fold, (t_idx, v_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "        print(f\"--- Fold {fold+1}/5 ---\")\n",
    "        Xt, Xv = X.iloc[t_idx], X.iloc[v_idx]\n",
    "        yt, yv = y.iloc[t_idx], y.iloc[v_idx]\n",
    "\n",
    "        gr_t = Xt['id2'].value_counts().sort_index().values\n",
    "        gr_v = Xv['id2'].value_counts().sort_index().values\n",
    "\n",
    "        Xt_, Xv_ = Xt.drop('id2', axis=1), Xv.drop('id2', axis=1)\n",
    "        Xtest_ = X_test.drop('id2', axis=1)\n",
    "\n",
    "        model_lgb = lgb.LGBMRanker(objective='lambdarank', metric='map', eval_at=7, learning_rate=0.03, n_estimators=500, num_leaves=50)\n",
    "        model_lgb.fit(Xt_, yt, group=gr_t, eval_set=[(Xv_, yv)], eval_group=[gr_v], callbacks=[lgb.early_stopping(50)])\n",
    "        oof_lgb[v_idx] = model_lgb.predict(Xv_)\n",
    "        test_lgb += model_lgb.predict(Xtest_)\n",
    "\n",
    "        model_xgb = XGBRanker(objective='rank:pairwise', learning_rate=0.03, n_estimators=500, max_depth=6)\n",
    "        model_xgb.fit(Xt_, yt, group=gr_t, eval_set=[(Xv_, yv)], eval_group=[gr_v], early_stopping_rounds=50, verbose=False)\n",
    "        oof_xgb[v_idx] = model_xgb.predict(Xv_)\n",
    "        test_xgb += model_xgb.predict(Xtest_)\n",
    "\n",
    "    test_lgb /= gkf.get_n_splits()\n",
    "    test_xgb /= gkf.get_n_splits()\n",
    "\n",
    "    oof_ensemble = 0.6 * oof_lgb + 0.4 * oof_xgb\n",
    "    test_ensemble = 0.6 * test_lgb + 0.4 * test_xgb\n",
    "\n",
    "    return oof_ensemble, test_ensemble\n",
    "\n",
    "# ===================================================================================\n",
    "# 5. Inference and Submission\n",
    "# ===================================================================================\n",
    "oof, test_preds = train_ensemble(X, y, X['id2'], X_test)\n",
    "train['pred'] = oof\n",
    "\n",
    "oof_ranked = train.sort_values(['id2','pred'], ascending=[True,False]).groupby('id2')['id3'].apply(list).reset_index(name='predicted_offers')\n",
    "true_offers = train[train['y']==1].groupby('id2')['id3'].apply(list).reset_index(name='true_offers')\n",
    "eval_df = oof_ranked.merge(true_offers, on='id2', how='left')\n",
    "eval_df['true_offers'] = eval_df['true_offers'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "final_score = mapk(eval_df['true_offers'], eval_df['predicted_offers'])\n",
    "print(f\"\\n📊 Final OOF MAP@7 Score: {final_score:.6f}\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "test['pred'] = scaler.fit_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "submission = test[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "submission['id5'] = pd.to_datetime(submission['id5']).dt.strftime('%m/%d/%Y')\n",
    "submission.to_csv('submissionnew.csv', index=False)\n",
    "print(\"\\n✅ Submission file saved as 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806969e-fc93-42f4-9f76-8c064c6b0bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f882973e-a4bd-4c19-b463-acad47c01c3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:218\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 218\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(left, right)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     72\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 88\u001b[0m\n\u001b[0;32m     84\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmerge(trans_agg, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid2\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 88\u001b[0m train \u001b[38;5;241m=\u001b[39m feature_engineer(train)\n\u001b[0;32m     89\u001b[0m test \u001b[38;5;241m=\u001b[39m feature_engineer(test)\n\u001b[0;32m     91\u001b[0m feature_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m train\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m train[c]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m [np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mint32, np\u001b[38;5;241m.\u001b[39mint64]] \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimp_dow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffer_duration\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_customer_imps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_customer_clicks\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdays_since_last_any_click\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_industry_imps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_industry_clicks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_spend\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[5], line 62\u001b[0m, in \u001b[0;36mfeature_engineer\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     60\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimp_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid5\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     61\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimp_dow\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimp_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdayofweek\n\u001b[1;32m---> 62\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffer_duration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdays\n\u001b[0;32m     64\u001b[0m events[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhas_clicked\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m events[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclick_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     65\u001b[0m cust_agg \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m     66\u001b[0m     total_customer_imps\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimpression_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     67\u001b[0m     total_customer_clicks\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhas_clicked\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     68\u001b[0m     last_click_date\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclick_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     69\u001b[0m )\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:194\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arith_method(other, operator\u001b[38;5;241m.\u001b[39msub)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6135\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[0;32m   6134\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[1;32m-> 6135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base\u001b[38;5;241m.\u001b[39mIndexOpsMixin\u001b[38;5;241m.\u001b[39m_arith_method(\u001b[38;5;28mself\u001b[39m, other, op)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:1382\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1382\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:283\u001b[0m, in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    279\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m--> 283\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(left, right, op)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:227\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    221\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m    222\u001b[0m     ):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m         result \u001b[38;5;241m=\u001b[39m _masked_arith_op(left, right, op)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:163\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[1;34m(x, y, op)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 163\u001b[0m         result[mask] \u001b[38;5;241m=\u001b[39m op(xrav[mask], yrav[mask])\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(y):\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRanker\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===================================================================================\n",
    "# 1. Custom MAP@7 Metric\n",
    "# ===================================================================================\n",
    "def mapk(actuals, predicteds, k=7):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actuals, predicteds)])\n",
    "\n",
    "def apk(actual, predicted, k=7):\n",
    "    if len(predicted) > k: predicted = predicted[:k]\n",
    "    score, num_hits = 0.0, 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    if not actual: return 0.0\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# ===================================================================================\n",
    "# 2. Load and Prepare Data\n",
    "# ===================================================================================\n",
    "train = pd.read_parquet('train_data.parquet')\n",
    "test = pd.read_parquet('test_data.parquet')\n",
    "offer_meta = pd.read_parquet('offer_metadata.parquet')\n",
    "events = pd.read_parquet('add_event.parquet')\n",
    "trans = pd.read_parquet('add_trans.parquet')\n",
    "\n",
    "for df in [train, test]:\n",
    "    for col in df.columns:\n",
    "        if col.startswith('f'):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df['id5'] = pd.to_datetime(df['id5'], errors='coerce')\n",
    "    if 'y' in df.columns:\n",
    "        df['y'] = pd.to_numeric(df['y'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "for df in [train, test, offer_meta, events, trans]:\n",
    "    for col in ['id2', 'id3', 'customer_id', 'offer_id']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "offer_meta.rename(columns={'id3': 'offer_id', 'id9': 'industry_code', 'id12': 'start_date', 'id13': 'end_date'}, inplace=True)\n",
    "events.rename(columns={'id2': 'customer_id', 'id3': 'offer_id', 'id4': 'impression_timestamp', 'id7': 'click_timestamp'}, inplace=True)\n",
    "trans.rename(columns={'id2': 'customer_id', 'f367': 'transaction_amount', 'f370': 'transaction_date_d', 'f371': 'transaction_date_t'}, inplace=True)\n",
    "trans['transaction_date'] = pd.to_datetime(trans['transaction_date_d'] + ' ' + trans['transaction_date_t'], errors='coerce')\n",
    "\n",
    "# ===================================================================================\n",
    "# 3. Feature Engineering\n",
    "# ===================================================================================\n",
    "def feature_engineer(df):\n",
    "    df = df.merge(offer_meta, left_on='id3', right_on='offer_id', how='left')\n",
    "    df['imp_date'] = pd.to_datetime(df['id5'])\n",
    "    df['imp_dow'] = df['imp_date'].dt.dayofweek\n",
    "    df['offer_duration'] = (df['end_date'] - df['start_date']).dt.days\n",
    "\n",
    "    events['has_clicked'] = events['click_timestamp'].notna().astype(int)\n",
    "    cust_agg = events.groupby('customer_id').agg(\n",
    "        total_customer_imps=('impression_timestamp', 'count'),\n",
    "        total_customer_clicks=('has_clicked', 'sum'),\n",
    "        last_click_date=('click_timestamp', 'max')\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "    df['days_since_last_any_click'] = (df['imp_date'] - df['last_click_date']).dt.days\n",
    "\n",
    "    cust_industry_agg = events.merge(offer_meta[['offer_id', 'industry_code']], on='offer_id')\n",
    "    cust_industry_agg = cust_industry_agg.groupby(['customer_id', 'industry_code']).agg(\n",
    "        customer_industry_imps=('impression_timestamp', 'count'),\n",
    "        customer_industry_clicks=('has_clicked', 'sum')\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_industry_agg, left_on=['id2', 'industry_code'], right_on=['customer_id', 'industry_code'], how='left')\n",
    "\n",
    "    trans_agg = trans.groupby('customer_id').agg(\n",
    "        avg_spend=('transaction_amount', 'mean'),\n",
    "        last_trans_date=('transaction_date', 'max')\n",
    "    ).reset_index()\n",
    "    df = df.merge(trans_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = feature_engineer(train)\n",
    "test = feature_engineer(test)\n",
    "\n",
    "feature_cols = [c for c in train.columns if c.startswith('f') and train[c].dtype in [np.float32, np.float64, np.int32, np.int64]] + [\n",
    "    'imp_dow', 'offer_duration', 'total_customer_imps', 'total_customer_clicks',\n",
    "    'days_since_last_any_click', 'customer_industry_imps', 'customer_industry_clicks', 'avg_spend']\n",
    "\n",
    "train.fillna(0, inplace=True)\n",
    "test.fillna(0, inplace=True)\n",
    "\n",
    "X = train[feature_cols + ['id2']]\n",
    "y = train['y']\n",
    "X_test = test[feature_cols + ['id2']]\n",
    "\n",
    "# ===================================================================================\n",
    "# 4. Ensemble Training with Hyperparameter Tuning\n",
    "# ===================================================================================\n",
    "def train_ensemble(X, y, groups, X_test):\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    oof_lgb, oof_xgb = np.zeros(len(X)), np.zeros(len(X))\n",
    "    test_lgb, test_xgb = np.zeros(len(X_test)), np.zeros(len(X_test))\n",
    "\n",
    "    for fold, (t_idx, v_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "        Xt, Xv = X.iloc[t_idx], X.iloc[v_idx]\n",
    "        yt, yv = y.iloc[t_idx], y.iloc[v_idx]\n",
    "\n",
    "        gr_t = Xt['id2'].value_counts().sort_index().values\n",
    "        gr_v = Xv['id2'].value_counts().sort_index().values\n",
    "\n",
    "        Xt_, Xv_ = Xt.drop('id2', axis=1), Xv.drop('id2', axis=1)\n",
    "        Xtest_ = X_test.drop('id2', axis=1)\n",
    "\n",
    "        model_lgb = lgb.LGBMRanker(objective='lambdarank', metric='map', eval_at=7, n_estimators=1000)\n",
    "        model_lgb.fit(Xt_, yt, group=gr_t, eval_set=[(Xv_, yv)], eval_group=[gr_v], callbacks=[lgb.early_stopping(100)])\n",
    "        oof_lgb[v_idx] = model_lgb.predict(Xv_)\n",
    "        test_lgb += model_lgb.predict(Xtest_)\n",
    "\n",
    "        model_xgb = XGBRanker(objective='rank:pairwise', learning_rate=0.03, n_estimators=1000, max_depth=6)\n",
    "        model_xgb.fit(Xt_, yt, group=gr_t, eval_set=[(Xv_, yv)], eval_group=[gr_v], early_stopping_rounds=100, verbose=False)\n",
    "        oof_xgb[v_idx] = model_xgb.predict(Xv_)\n",
    "        test_xgb += model_xgb.predict(Xtest_)\n",
    "\n",
    "    test_lgb /= gkf.get_n_splits()\n",
    "    test_xgb /= gkf.get_n_splits()\n",
    "\n",
    "    oof_ensemble = 0.6 * oof_lgb + 0.4 * oof_xgb\n",
    "    test_ensemble = 0.6 * test_lgb + 0.4 * test_xgb\n",
    "\n",
    "    return oof_ensemble, test_ensemble\n",
    "\n",
    "# ===================================================================================\n",
    "# 5. Inference and Submission\n",
    "# ===================================================================================\n",
    "oof, test_preds = train_ensemble(X, y, X['id2'], X_test)\n",
    "train['pred'] = oof\n",
    "\n",
    "oof_ranked = train.sort_values(['id2','pred'], ascending=[True,False]).groupby('id2')['id3'].apply(list).reset_index(name='predicted_offers')\n",
    "true_offers = train[train['y']==1].groupby('id2')['id3'].apply(list).reset_index(name='true_offers')\n",
    "eval_df = oof_ranked.merge(true_offers, on='id2', how='left')\n",
    "eval_df['true_offers'] = eval_df['true_offers'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "final_score = mapk(eval_df['true_offers'], eval_df['predicted_offers'])\n",
    "print(f\"\\n📊 Final OOF MAP@7 Score: {final_score:.6f}\")\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "test['pred'] = scaler.fit_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "submission = test[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "submission['id5'] = pd.to_datetime(submission['id5']).dt.strftime('%m/%d/%Y')\n",
    "submission.to_csv('submissionnewd.csv', index=False)\n",
    "print(\"\\n✅ Submission file saved as 'submission.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e2a6463-3d0c-40e0-920c-a31234319a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import ndcg_score\n",
    "import matplotlib.pyplot as plt\n",
    "import gc, os, warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===================================================================================\n",
    "# 0. MAP@7 Metric\n",
    "# ===================================================================================\n",
    "def mapk(actuals, predicteds, k=7):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actuals, predicteds)])\n",
    "\n",
    "def apk(actual, predicted, k=7):\n",
    "    if len(predicted) > k: predicted = predicted[:k]\n",
    "    score, num_hits = 0.0, 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    if not actual: return 0.0\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# ===================================================================================\n",
    "# 1. Load and Feature Functions (Assume already implemented correctly)\n",
    "# ===================================================================================\n",
    "# Use your own load_and_clean_data() and feature_engineer() from previous step.\n",
    "\n",
    "# ===================================================================================\n",
    "# 2. Optuna-based Hyperparameter Tuning\n",
    "# ===================================================================================\n",
    "def objective(trial, X, y, groups):\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'map',\n",
    "        'eval_at': 7,\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int(\"num_leaves\", 31, 150),\n",
    "        'min_child_samples': trial.suggest_int(\"min_child_samples\", 10, 100),\n",
    "        'subsample': trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float(\"colsample_bytree\", 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float(\"reg_alpha\", 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float(\"reg_lambda\", 0.0, 1.0),\n",
    "    }\n",
    "\n",
    "    gkf = GroupKFold(n_splits=3)\n",
    "    score_list = []\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "        Xt, Xv = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        yt, yv = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        group_train = Xt.groupby('id2').size().values\n",
    "        group_valid = Xv.groupby('id2').size().values\n",
    "        Xt_, Xv_ = Xt.drop('id2', axis=1), Xv.drop('id2', axis=1)\n",
    "\n",
    "        model = lgb.LGBMRanker(**params, n_estimators=2000)\n",
    "        model.fit(Xt_, yt, group=group_train,\n",
    "                  eval_set=[(Xv_, yv)],\n",
    "                  eval_group=[group_valid],\n",
    "                  early_stopping_rounds=50, verbose=False)\n",
    "        preds = model.predict(Xv_)\n",
    "        Xv['pred'] = preds\n",
    "        pred_ranked = Xv.sort_values(['id2', 'pred'], ascending=[True, False]).groupby('id2')['id3'].apply(list).reset_index(name='predicted_offers')\n",
    "        true_ranked = Xv[Xv['y'] == 1].groupby('id2')['id3'].apply(list).reset_index(name='true_offers')\n",
    "        eval_df = pred_ranked.merge(true_ranked, on='id2', how='left')\n",
    "        eval_df['true_offers'] = eval_df['true_offers'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "        map7 = mapk(eval_df['true_offers'], eval_df['predicted_offers'])\n",
    "        score_list.append(map7)\n",
    "        del model; gc.collect()\n",
    "\n",
    "    return np.mean(score_list)\n",
    "\n",
    "def tune_hyperparams(X, y, groups, n_trials=30):\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, X, y, groups), n_trials=n_trials)\n",
    "    print(\"Best Params:\", study.best_params)\n",
    "    return study.best_params\n",
    "\n",
    "# ===================================================================================\n",
    "# 3. Training with Ensemble\n",
    "# ===================================================================================\n",
    "def train_and_predict(X, y, X_test, groups, best_params=None, n_folds=5):\n",
    "    oof = np.zeros(len(X))\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "    feature_importances = pd.DataFrame()\n",
    "    feature_importances[\"feature\"] = X.drop('id2', axis=1).columns\n",
    "\n",
    "    gkf = GroupKFold(n_splits=n_folds)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "        print(f\"\\n🔁 Fold {fold+1}/{n_folds}\")\n",
    "        Xt, Xv = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        yt, yv = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        group_train = Xt.groupby('id2').size().values\n",
    "        group_valid = Xv.groupby('id2').size().values\n",
    "\n",
    "        Xt_, Xv_ = Xt.drop('id2', axis=1), Xv.drop('id2', axis=1)\n",
    "        Xtest_ = X_test.drop('id2', axis=1)\n",
    "\n",
    "        model = lgb.LGBMRanker(**best_params, n_estimators=3000)\n",
    "        model.fit(Xt_, yt, group=group_train, eval_set=[(Xv_, yv)],\n",
    "                  eval_group=[group_valid], early_stopping_rounds=100, verbose=100)\n",
    "\n",
    "        oof[valid_idx] = model.predict(Xv_)\n",
    "        test_preds += model.predict(Xtest_)\n",
    "        feature_importances[f'fold_{fold+1}'] = model.feature_importances_\n",
    "\n",
    "        del model; gc.collect()\n",
    "\n",
    "    test_preds /= n_folds\n",
    "    return oof, test_preds, feature_importances\n",
    "\n",
    "# ===================================================================================\n",
    "# 4. Visualization\n",
    "# ===================================================================================\n",
    "def plot_feature_importance(importance_df):\n",
    "    importance_df['mean_importance'] = importance_df.drop('feature', axis=1).mean(axis=1)\n",
    "    top_feats = importance_df.sort_values(by='mean_importance', ascending=False).head(20)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(top_feats['feature'], top_feats['mean_importance'])\n",
    "    plt.title(\"Top 20 Feature Importances\")\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ===================================================================================\n",
    "# 5. Main Execution\n",
    "# ===================================================================================\n",
    "if __name__ == '_main_':\n",
    "    print(\"📂 Loading data and features...\")\n",
    "    train_df, test_df, offer_meta, events, trans = load_and_clean_data()\n",
    "    train_df = feature_engineer(train_df, offer_meta, events, trans)\n",
    "    test_df = feature_engineer(test_df, offer_meta, events, trans)\n",
    "\n",
    "    train_df.fillna(0, inplace=True)\n",
    "    test_df.fillna(0, inplace=True)\n",
    "\n",
    "    numeric_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    feature_cols = list(set(numeric_cols) - set(['y', 'id1', 'id2', 'id3']))\n",
    "    X = train_df[feature_cols + ['id2']]\n",
    "    y = train_df['y']\n",
    "    X_test = test_df[feature_cols + ['id2']]\n",
    "\n",
    "    print(\"🎯 Tuning hyperparameters...\")\n",
    "    best_params = tune_hyperparams(X, y, X['id2'], n_trials=30)\n",
    "    print(\"✅ Best Params:\", best_params)\n",
    "\n",
    "    print(\"🧠 Training and predicting...\")\n",
    "    oof_preds, test_preds, fi_df = train_and_predict(X, y, X_test, X['id2'], best_params)\n",
    "\n",
    "    print(\"📊 Ranking and evaluating...\")\n",
    "    train_df['pred'] = oof_preds\n",
    "    oof_ranked = train_df.sort_values(['id2', 'pred'], ascending=[True, False]).groupby('id2')['id3'].apply(list).reset_index(name='predicted_offers')\n",
    "    true_offers = train_df[train_df['y'] == 1].groupby('id2')['id3'].apply(list).reset_index(name='true_offers')\n",
    "    eval_df = oof_ranked.merge(true_offers, on='id2', how='left')\n",
    "    eval_df['true_offers'] = eval_df['true_offers'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    final_map = mapk(eval_df['true_offers'], eval_df['predicted_offers'])\n",
    "    print(f\"📈 Final OOF MAP@7 = {final_map:.6f}\")\n",
    "\n",
    "    print(\"📦 Creating submission...\")\n",
    "    scaler = MinMaxScaler()\n",
    "    test_df['pred'] = scaler.fit_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "    submission = test_df[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "    submission['id5'] = pd.to_datetime(submission['id5']).dt.strftime('%m/%d/%Y')\n",
    "    submission.to_csv('submissiondcgyidc.csv', index=False)\n",
    "    print(\"✅ Submission saved to submission.csv\")\n",
    "\n",
    "    print(\"📉 Plotting feature importance...\")\n",
    "    plot_feature_importance(fi_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00c93eb6-fd8c-46be-8820-b6c894f8e6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: alembic>=1.5.0 in c:\\users\\fahee\\anaconda3\\lib\\site-packages (from optuna) (1.13.3)\n",
      "Collecting colorlog (from optuna)\n",
      "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\fahee\\anaconda3\\lib\\site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\fahee\\anaconda3\\lib\\site-packages (from optuna) (24.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in c:\\users\\fahee\\anaconda3\\lib\\site-packages (from optuna) (2.0.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\fahee\\anaconda3\\lib\\site-packages (from optuna) (4.66.5)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\fahee\\anaconda3\\lib\\site-packages (from optuna) (6.0.1)\n",
      "Requirement already satisfied: Mako in c:\\users\\fahee\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (1.2.3)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\fahee\\anaconda3\\lib\\site-packages (from alembic>=1.5.0->optuna) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\fahee\\anaconda3\\lib\\site-packages (from sqlalchemy>=1.4.2->optuna) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\fahee\\anaconda3\\lib\\site-packages (from colorlog->optuna) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\fahee\\anaconda3\\lib\\site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
      "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
      "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: colorlog, optuna\n",
      "Successfully installed colorlog-6.9.0 optuna-4.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c0bbaa-ee81-4701-b3a1-573814c8e002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading datasets...\n",
      "🔧 Preprocessing features...\n",
      "🛠️  Feature engineering...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:218\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 218\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(left, right)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate(op, op_str, a, b)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     72\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op(a, b)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 91\u001b[0m\n\u001b[0;32m     87\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mmerge(trans_agg, left_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid2\u001b[39m\u001b[38;5;124m'\u001b[39m, right_on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[1;32m---> 91\u001b[0m train \u001b[38;5;241m=\u001b[39m feature_engineer(train)\n\u001b[0;32m     92\u001b[0m test \u001b[38;5;241m=\u001b[39m feature_engineer(test)\n\u001b[0;32m     94\u001b[0m feature_cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m train\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m train[c]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m [np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mint32, np\u001b[38;5;241m.\u001b[39mint64]] \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimp_dow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffer_duration\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_customer_imps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_customer_clicks\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdays_since_last_any_click\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_industry_imps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_industry_clicks\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_spend\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[1], line 65\u001b[0m, in \u001b[0;36mfeature_engineer\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     63\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimp_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid5\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     64\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimp_dow\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimp_date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdayofweek\n\u001b[1;32m---> 65\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffer_duration\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mend_date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m-\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdays\n\u001b[0;32m     67\u001b[0m events[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhas_clicked\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m events[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclick_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     68\u001b[0m cust_agg \u001b[38;5;241m=\u001b[39m events\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcustomer_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[0;32m     69\u001b[0m     total_customer_imps\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimpression_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     70\u001b[0m     total_customer_clicks\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhas_clicked\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     71\u001b[0m     last_click_date\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclick_timestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     72\u001b[0m )\u001b[38;5;241m.\u001b[39mreset_index()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\arraylike.py:194\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_arith_method(other, operator\u001b[38;5;241m.\u001b[39msub)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:6135\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[0;32m   6134\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[1;32m-> 6135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base\u001b[38;5;241m.\u001b[39mIndexOpsMixin\u001b[38;5;241m.\u001b[39m_arith_method(\u001b[38;5;28mself\u001b[39m, other, op)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:1382\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1379\u001b[0m     rvalues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(rvalues\u001b[38;5;241m.\u001b[39mstart, rvalues\u001b[38;5;241m.\u001b[39mstop, rvalues\u001b[38;5;241m.\u001b[39mstep)\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1382\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(result, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:283\u001b[0m, in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    279\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[1;32m--> 283\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m _na_arithmetic_op(left, right, op)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:227\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    221\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m    222\u001b[0m     ):\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[1;32m--> 227\u001b[0m         result \u001b[38;5;241m=\u001b[39m _masked_arith_op(left, right, op)\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:163\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[1;34m(x, y, op)\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m--> 163\u001b[0m         result[mask] \u001b[38;5;241m=\u001b[39m op(xrav[mask], yrav[mask])\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(y):\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRanker\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===================================================================================\n",
    "# 1. Custom MAP@7 Metric\n",
    "# ===================================================================================\n",
    "def mapk(actuals, predicteds, k=7):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actuals, predicteds)])\n",
    "\n",
    "def apk(actual, predicted, k=7):\n",
    "    if len(predicted) > k: predicted = predicted[:k]\n",
    "    score, num_hits = 0.0, 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    if not actual: return 0.0\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# ===================================================================================\n",
    "# 2. Load and Prepare Data\n",
    "# ===================================================================================\n",
    "print(\"📥 Loading datasets...\")\n",
    "train = pd.read_parquet('train_data.parquet')\n",
    "test = pd.read_parquet('test_data.parquet')\n",
    "offer_meta = pd.read_parquet('offer_metadata.parquet')\n",
    "events = pd.read_parquet('add_event.parquet')\n",
    "trans = pd.read_parquet('add_trans.parquet')\n",
    "\n",
    "print(\"🔧 Preprocessing features...\")\n",
    "for df in [train, test]:\n",
    "    for col in df.columns:\n",
    "        if col.startswith('f'):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df['id5'] = pd.to_datetime(df['id5'], errors='coerce')\n",
    "    if 'y' in df.columns:\n",
    "        df['y'] = pd.to_numeric(df['y'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "for df in [train, test, offer_meta, events, trans]:\n",
    "    for col in ['id2', 'id3', 'customer_id', 'offer_id']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "offer_meta.rename(columns={'id3': 'offer_id', 'id9': 'industry_code', 'id12': 'start_date', 'id13': 'end_date'}, inplace=True)\n",
    "events.rename(columns={'id2': 'customer_id', 'id3': 'offer_id', 'id4': 'impression_timestamp', 'id7': 'click_timestamp'}, inplace=True)\n",
    "trans.rename(columns={'id2': 'customer_id', 'f367': 'transaction_amount', 'f370': 'transaction_date_d', 'f371': 'transaction_date_t'}, inplace=True)\n",
    "trans['transaction_date'] = pd.to_datetime(trans['transaction_date_d'] + ' ' + trans['transaction_date_t'], errors='coerce')\n",
    "\n",
    "# ===================================================================================\n",
    "# 3. Feature Engineering\n",
    "# ===================================================================================\n",
    "def feature_engineer(df):\n",
    "    print(\"🛠️  Feature engineering...\")\n",
    "    df = df.merge(offer_meta, left_on='id3', right_on='offer_id', how='left')\n",
    "    df['imp_date'] = pd.to_datetime(df['id5'])\n",
    "    df['imp_dow'] = df['imp_date'].dt.dayofweek\n",
    "    df['offer_duration'] = (df['end_date'] - df['start_date']).dt.days\n",
    "\n",
    "    events['has_clicked'] = events['click_timestamp'].notna().astype(int)\n",
    "    cust_agg = events.groupby('customer_id').agg(\n",
    "        total_customer_imps=('impression_timestamp', 'count'),\n",
    "        total_customer_clicks=('has_clicked', 'sum'),\n",
    "        last_click_date=('click_timestamp', 'max')\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "    df['days_since_last_any_click'] = (df['imp_date'] - df['last_click_date']).dt.days\n",
    "\n",
    "    cust_industry_agg = events.merge(offer_meta[['offer_id', 'industry_code']], on='offer_id')\n",
    "    cust_industry_agg = cust_industry_agg.groupby(['customer_id', 'industry_code']).agg(\n",
    "        customer_industry_imps=('impression_timestamp', 'count'),\n",
    "        customer_industry_clicks=('has_clicked', 'sum')\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_industry_agg, left_on=['id2', 'industry_code'], right_on=['customer_id', 'industry_code'], how='left')\n",
    "\n",
    "    trans_agg = trans.groupby('customer_id').agg(\n",
    "        avg_spend=('transaction_amount', 'mean'),\n",
    "        last_trans_date=('transaction_date', 'max')\n",
    "    ).reset_index()\n",
    "    df = df.merge(trans_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = feature_engineer(train)\n",
    "test = feature_engineer(test)\n",
    "\n",
    "feature_cols = [c for c in train.columns if c.startswith('f') and train[c].dtype in [np.float32, np.float64, np.int32, np.int64]] + [\n",
    "    'imp_dow', 'offer_duration', 'total_customer_imps', 'total_customer_clicks',\n",
    "    'days_since_last_any_click', 'customer_industry_imps', 'customer_industry_clicks', 'avg_spend']\n",
    "\n",
    "train.fillna(0, inplace=True)\n",
    "test.fillna(0, inplace=True)\n",
    "\n",
    "X = train[feature_cols + ['id2']]\n",
    "y = train['y']\n",
    "X_test = test[feature_cols + ['id2']]\n",
    "\n",
    "# ===================================================================================\n",
    "# 4. Ensemble Training with Hyperparameter Tuning\n",
    "# ===================================================================================\n",
    "def train_ensemble(X, y, groups, X_test):\n",
    "    print(\"📈 Training ensemble model...\")\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    oof_lgb, oof_xgb = np.zeros(len(X)), np.zeros(len(X))\n",
    "    test_lgb, test_xgb = np.zeros(len(X_test)), np.zeros(len(X_test))\n",
    "\n",
    "    for fold, (t_idx, v_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "        print(f\"\\n🔁 Fold {fold+1}/5\")\n",
    "        Xt, Xv = X.iloc[t_idx], X.iloc[v_idx]\n",
    "        yt, yv = y.iloc[t_idx], y.iloc[v_idx]\n",
    "\n",
    "        gr_t = Xt['id2'].value_counts().sort_index().values\n",
    "        gr_v = Xv['id2'].value_counts().sort_index().values\n",
    "\n",
    "        Xt_, Xv_ = Xt.drop('id2', axis=1), Xv.drop('id2', axis=1)\n",
    "        Xtest_ = X_test.drop('id2', axis=1)\n",
    "\n",
    "        model_lgb = lgb.LGBMRanker(objective='lambdarank', metric='map', eval_at=7, n_estimators=500, learning_rate=0.05, num_leaves=80)\n",
    "        model_lgb.fit(Xt_, yt, group=gr_t, eval_set=[(Xv_, yv)], eval_group=[gr_v], callbacks=[lgb.early_stopping(50)])\n",
    "        oof_lgb[v_idx] = model_lgb.predict(Xv_)\n",
    "        test_lgb += model_lgb.predict(Xtest_)\n",
    "\n",
    "        model_xgb = XGBRanker(objective='rank:pairwise', learning_rate=0.05, n_estimators=500, max_depth=6, verbosity=0)\n",
    "        model_xgb.fit(Xt_, yt, group=gr_t, eval_set=[(Xv_, yv)], eval_group=[gr_v], early_stopping_rounds=50)\n",
    "        oof_xgb[v_idx] = model_xgb.predict(Xv_)\n",
    "        test_xgb += model_xgb.predict(Xtest_)\n",
    "\n",
    "    test_lgb /= gkf.get_n_splits()\n",
    "    test_xgb /= gkf.get_n_splits()\n",
    "\n",
    "    print(\"🔗 Blending predictions...\")\n",
    "    oof_ensemble = 0.6 * oof_lgb + 0.4 * oof_xgb\n",
    "    test_ensemble = 0.6 * test_lgb + 0.4 * test_xgb\n",
    "\n",
    "    return oof_ensemble, test_ensemble\n",
    "\n",
    "# ===================================================================================\n",
    "# 5. Inference and Submission\n",
    "# ===================================================================================\n",
    "oof, test_preds = train_ensemble(X, y, X['id2'], X_test)\n",
    "train['pred'] = oof\n",
    "\n",
    "print(\"📤 Generating predictions and evaluating MAP@7...\")\n",
    "oof_ranked = train.sort_values(['id2','pred'], ascending=[True,False]).groupby('id2')['id3'].apply(list).reset_index(name='predicted_offers')\n",
    "true_offers = train[train['y']==1].groupby('id2')['id3'].apply(list).reset_index(name='true_offers')\n",
    "eval_df = oof_ranked.merge(true_offers, on='id2', how='left')\n",
    "eval_df['true_offers'] = eval_df['true_offers'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "final_score = mapk(eval_df['true_offers'], eval_df['predicted_offers'])\n",
    "print(f\"\\n📊 Final OOF MAP@7 Score: {final_score:.6f}\")\n",
    "\n",
    "print(\"🧪 Generating submission file...\")\n",
    "scaler = MinMaxScaler()\n",
    "test['pred'] = scaler.fit_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "submission = test[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "submission['id5'] = pd.to_datetime(submission['id5']).dt.strftime('%m/%d/%Y')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\n✅ Submission file saved as 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826cdd62-188e-435e-a25b-1e23c5628800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading datasets...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost import XGBRanker\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===================================================================================\n",
    "# 1. Custom MAP@7 Metric\n",
    "# ===================================================================================\n",
    "def mapk(actuals, predicteds, k=7):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actuals, predicteds)])\n",
    "\n",
    "def apk(actual, predicted, k=7):\n",
    "    if len(predicted) > k: predicted = predicted[:k]\n",
    "    score, num_hits = 0.0, 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    if not actual: return 0.0\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# ===================================================================================\n",
    "# 2. Load and Prepare Data\n",
    "# ===================================================================================\n",
    "print(\"📥 Loading datasets...\")\n",
    "train = pd.read_parquet('train_data.parquet')\n",
    "test = pd.read_parquet('test_data.parquet')\n",
    "offer_meta = pd.read_parquet('offer_metadata.parquet')\n",
    "events = pd.read_parquet('add_event.parquet')\n",
    "trans = pd.read_parquet('add_trans.parquet')\n",
    "\n",
    "print(\"🔧 Preprocessing features...\")\n",
    "for df in [train, test]:\n",
    "    for col in df.columns:\n",
    "        if col.startswith('f'):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df['id5'] = pd.to_datetime(df['id5'], errors='coerce')\n",
    "    if 'y' in df.columns:\n",
    "        df['y'] = pd.to_numeric(df['y'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "for df in [train, test, offer_meta, events, trans]:\n",
    "    for col in ['id2', 'id3', 'customer_id', 'offer_id']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "offer_meta.rename(columns={'id3': 'offer_id', 'id9': 'industry_code', 'id12': 'start_date', 'id13': 'end_date'}, inplace=True)\n",
    "events.rename(columns={'id2': 'customer_id', 'id3': 'offer_id', 'id4': 'impression_timestamp', 'id7': 'click_timestamp'}, inplace=True)\n",
    "trans.rename(columns={'id2': 'customer_id', 'f367': 'transaction_amount', 'f370': 'transaction_date_d', 'f371': 'transaction_date_t'}, inplace=True)\n",
    "trans['transaction_date'] = pd.to_datetime(trans['transaction_date_d'] + ' ' + trans['transaction_date_t'], errors='coerce')\n",
    "\n",
    "# ===================================================================================\n",
    "# 3. Feature Engineering\n",
    "# ===================================================================================\n",
    "def feature_engineer(df):\n",
    "    print(\"🛠️  Feature engineering...\")\n",
    "    df = df.merge(offer_meta, left_on='id3', right_on='offer_id', how='left')\n",
    "    df['imp_date'] = pd.to_datetime(df['id5'])\n",
    "    df['imp_dow'] = df['imp_date'].dt.dayofweek\n",
    "    df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "    df['end_date'] = pd.to_datetime(df['end_date'], errors='coerce')\n",
    "    df['offer_duration'] = (df['end_date'] - df['start_date']).dt.days\n",
    "\n",
    "    events['click_timestamp'] = pd.to_datetime(events['click_timestamp'], errors='coerce')\n",
    "    events['impression_timestamp'] = pd.to_datetime(events['impression_timestamp'], errors='coerce')\n",
    "    events['has_clicked'] = events['click_timestamp'].notna().astype(int)\n",
    "\n",
    "    cust_agg = events.groupby('customer_id').agg(\n",
    "        total_customer_imps=('impression_timestamp', 'count'),\n",
    "        total_customer_clicks=('has_clicked', 'sum'),\n",
    "        last_click_date=('click_timestamp', lambda x: pd.to_datetime(x).max())\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "    df['days_since_last_any_click'] = (df['imp_date'] - df['last_click_date']).dt.days\n",
    "\n",
    "    cust_industry_agg = events.merge(offer_meta[['offer_id', 'industry_code']], on='offer_id')\n",
    "    cust_industry_agg = cust_industry_agg.groupby(['customer_id', 'industry_code']).agg(\n",
    "        customer_industry_imps=('impression_timestamp', 'count'),\n",
    "        customer_industry_clicks=('has_clicked', 'sum')\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_industry_agg, left_on=['id2', 'industry_code'], right_on=['customer_id', 'industry_code'], how='left')\n",
    "\n",
    "    trans['transaction_amount'] = pd.to_numeric(trans['transaction_amount'], errors='coerce')\n",
    "    trans_agg = trans.groupby('customer_id').agg(\n",
    "        avg_spend=('transaction_amount', 'mean'),\n",
    "        last_trans_date=('transaction_date', lambda x: pd.to_datetime(x).max())\n",
    "    ).reset_index()\n",
    "    df = df.merge(trans_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = feature_engineer(train)\n",
    "test = feature_engineer(test)\n",
    "\n",
    "feature_cols = [c for c in train.columns if c.startswith('f') and train[c].dtype in [np.float32, np.float64, np.int32, np.int64]] + [\n",
    "    'imp_dow', 'offer_duration', 'total_customer_imps', 'total_customer_clicks',\n",
    "    'days_since_last_any_click', 'customer_industry_imps', 'customer_industry_clicks', 'avg_spend']\n",
    "\n",
    "train.fillna(0, inplace=True)\n",
    "test.fillna(0, inplace=True)\n",
    "\n",
    "X = train[feature_cols + ['id2']]\n",
    "y = train['y']\n",
    "X_test = test[feature_cols + ['id2']]\n",
    "\n",
    "# ===================================================================================\n",
    "# 4. Ensemble Training with Hyperparameter Tuning\n",
    "# ===================================================================================\n",
    "def train_ensemble(X, y, groups, X_test):\n",
    "    print(\"📈 Training ensemble model...\")\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    oof_lgb, oof_xgb = np.zeros(len(X)), np.zeros(len(X))\n",
    "    test_lgb, test_xgb = np.zeros(len(X_test)), np.zeros(len(X_test))\n",
    "\n",
    "    for fold, (t_idx, v_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "        print(f\"\\n🔁 Fold {fold+1}/5\")\n",
    "        Xt, Xv = X.iloc[t_idx], X.iloc[v_idx]\n",
    "        yt, yv = y.iloc[t_idx], y.iloc[v_idx]\n",
    "\n",
    "        gr_t = Xt['id2'].value_counts().sort_index().values\n",
    "        gr_v = Xv['id2'].value_counts().sort_index().values\n",
    "\n",
    "        Xt_, Xv_ = Xt.drop('id2', axis=1), Xv.drop('id2', axis=1)\n",
    "        Xtest_ = X_test.drop('id2', axis=1)\n",
    "\n",
    "        model_lgb = lgb.LGBMRanker(objective='lambdarank', metric='map', eval_at=7, n_estimators=500, learning_rate=0.05, num_leaves=80)\n",
    "        model_lgb.fit(Xt_, yt, group=gr_t, eval_set=[(Xv_, yv)], eval_group=[gr_v], callbacks=[lgb.early_stopping(50)])\n",
    "        oof_lgb[v_idx] = model_lgb.predict(Xv_)\n",
    "        test_lgb += model_lgb.predict(Xtest_)\n",
    "\n",
    "        model_xgb = XGBRanker(objective='rank:pairwise', learning_rate=0.05, n_estimators=500, max_depth=6, verbosity=0)\n",
    "        model_xgb.fit(Xt_, yt, group=gr_t)\n",
    "        oof_xgb[v_idx] = model_xgb.predict(Xv_)\n",
    "        test_xgb += model_xgb.predict(Xtest_)\n",
    "\n",
    "    test_lgb /= gkf.get_n_splits()\n",
    "    test_xgb /= gkf.get_n_splits()\n",
    "\n",
    "    print(\"🔗 Blending predictions...\")\n",
    "    oof_ensemble = 0.6 * oof_lgb + 0.4 * oof_xgb\n",
    "    test_ensemble = 0.6 * test_lgb + 0.4 * test_xgb\n",
    "\n",
    "    return oof_ensemble, test_ensemble\n",
    "\n",
    "# ===================================================================================\n",
    "# 5. Inference and Submission\n",
    "# ===================================================================================\n",
    "oof, test_preds = train_ensemble(X, y, X['id2'], X_test)\n",
    "train['pred'] = oof\n",
    "\n",
    "print(\"📤 Generating predictions and evaluating MAP@7...\")\n",
    "oof_ranked = train.sort_values(['id2','pred'], ascending=[True,False]).groupby('id2')['id3'].apply(list).reset_index(name='predicted_offers')\n",
    "true_offers = train[train['y']==1].groupby('id2')['id3'].apply(list).reset_index(name='true_offers')\n",
    "eval_df = oof_ranked.merge(true_offers, on='id2', how='left')\n",
    "eval_df['true_offers'] = eval_df['true_offers'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "final_score = mapk(eval_df['true_offers'], eval_df['predicted_offers'])\n",
    "print(f\"\\n📊 Final OOF MAP@7 Score: {final_score:.6f}\")\n",
    "\n",
    "print(\"🧪 Generating submission file...\")\n",
    "scaler = MinMaxScaler()\n",
    "test['pred'] = scaler.fit_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "submission = test[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "submission['id5'] = pd.to_datetime(submission['id5']).dt.strftime('%m/%d/%Y')\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"\\n✅ Submission file saved as 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78daa7ff-a672-4416-bd39-7be1a07e1887",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d160eace-c053-4246-92f9-72cb2139c15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a46dc9-f9cd-4f80-a338-afe685d56f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading datasets...\n",
      "🔧 Preprocessing features...\n",
      "🛠️  Feature engineering...\n",
      "🛠️  Feature engineering...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ===================================================================================\n",
    "# 1. Custom MAP@7 Metric\n",
    "# ===================================================================================\n",
    "def mapk(actuals, predicteds, k=7):\n",
    "    return np.mean([apk(a, p, k) for a, p in zip(actuals, predicteds)])\n",
    "\n",
    "def apk(actual, predicted, k=7):\n",
    "    if len(predicted) > k: predicted = predicted[:k]\n",
    "    score, num_hits = 0.0, 0.0\n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    if not actual: return 0.0\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "# ===================================================================================\n",
    "# 2. Load and Prepare Data\n",
    "# ===================================================================================\n",
    "print(\"📥 Loading datasets...\")\n",
    "train = pd.read_parquet('train_data.parquet')\n",
    "test = pd.read_parquet('test_data.parquet')\n",
    "offer_meta = pd.read_parquet('offer_metadata.parquet')\n",
    "events = pd.read_parquet('add_event.parquet')\n",
    "trans = pd.read_parquet('add_trans.parquet')\n",
    "\n",
    "print(\"🔧 Preprocessing features...\")\n",
    "for df in [train, test]:\n",
    "    for col in df.columns:\n",
    "        if col.startswith('f'):\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    df['id5'] = pd.to_datetime(df['id5'], errors='coerce')\n",
    "    if 'y' in df.columns:\n",
    "        df['y'] = pd.to_numeric(df['y'], errors='coerce').fillna(0).astype(float)\n",
    "\n",
    "for df in [train, test, offer_meta, events, trans]:\n",
    "    for col in ['id2', 'id3', 'customer_id', 'offer_id']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "offer_meta.rename(columns={'id3': 'offer_id', 'id9': 'industry_code', 'id12': 'start_date', 'id13': 'end_date'}, inplace=True)\n",
    "events.rename(columns={'id2': 'customer_id', 'id3': 'offer_id', 'id4': 'impression_timestamp', 'id7': 'click_timestamp'}, inplace=True)\n",
    "trans.rename(columns={'id2': 'customer_id', 'f367': 'transaction_amount', 'f370': 'transaction_date_d', 'f371': 'transaction_date_t'}, inplace=True)\n",
    "trans['transaction_date'] = pd.to_datetime(trans['transaction_date_d'] + ' ' + trans['transaction_date_t'], errors='coerce')\n",
    "\n",
    "# ===================================================================================\n",
    "# 3. Feature Engineering\n",
    "# ===================================================================================\n",
    "def feature_engineer(df):\n",
    "    print(\"🛠️  Feature engineering...\")\n",
    "    df = df.merge(offer_meta, left_on='id3', right_on='offer_id', how='left')\n",
    "    df['imp_date'] = pd.to_datetime(df['id5'])\n",
    "    df['imp_dow'] = df['imp_date'].dt.dayofweek\n",
    "    df['start_date'] = pd.to_datetime(df['start_date'], errors='coerce')\n",
    "    df['end_date'] = pd.to_datetime(df['end_date'], errors='coerce')\n",
    "    df['offer_duration'] = (df['end_date'] - df['start_date']).dt.days\n",
    "\n",
    "    events['click_timestamp'] = pd.to_datetime(events['click_timestamp'], errors='coerce')\n",
    "    events['impression_timestamp'] = pd.to_datetime(events['impression_timestamp'], errors='coerce')\n",
    "    events['has_clicked'] = events['click_timestamp'].notna().astype(int)\n",
    "\n",
    "    cust_agg = events.groupby('customer_id').agg(\n",
    "        total_customer_imps=('impression_timestamp', 'count'),\n",
    "        total_customer_clicks=('has_clicked', 'sum'),\n",
    "        last_click_date=('click_timestamp', lambda x: pd.to_datetime(x.dropna()).max())\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "    df['days_since_last_any_click'] = (df['imp_date'] - df['last_click_date']).dt.days\n",
    "\n",
    "    cust_industry_agg = events.merge(offer_meta[['offer_id', 'industry_code']], on='offer_id')\n",
    "    cust_industry_agg = cust_industry_agg.groupby(['customer_id', 'industry_code']).agg(\n",
    "        customer_industry_imps=('impression_timestamp', 'count'),\n",
    "        customer_industry_clicks=('has_clicked', 'sum')\n",
    "    ).reset_index()\n",
    "    df = df.merge(cust_industry_agg, left_on=['id2', 'industry_code'], right_on=['customer_id', 'industry_code'], how='left')\n",
    "\n",
    "    trans['transaction_amount'] = pd.to_numeric(trans['transaction_amount'], errors='coerce')\n",
    "    trans_agg = trans.groupby('customer_id').agg(\n",
    "        avg_spend=('transaction_amount', 'mean'),\n",
    "        last_trans_date=('transaction_date', lambda x: pd.to_datetime(x.dropna()).max())\n",
    "    ).reset_index()\n",
    "    df = df.merge(trans_agg, left_on='id2', right_on='customer_id', how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "train = feature_engineer(train)\n",
    "test = feature_engineer(test)\n",
    "\n",
    "feature_cols = [c for c in train.columns if c.startswith('f') and train[c].dtype in [np.float32, np.float64, np.int32, np.int64]] + [\n",
    "    'imp_dow', 'offer_duration', 'total_customer_imps', 'total_customer_clicks',\n",
    "    'days_since_last_any_click', 'customer_industry_imps', 'customer_industry_clicks', 'avg_spend']\n",
    "\n",
    "train.fillna(0, inplace=True)\n",
    "test.fillna(0, inplace=True)\n",
    "\n",
    "X = train[feature_cols + ['id2']]\n",
    "y = train['y']\n",
    "X_test = test[feature_cols + ['id2']]\n",
    "\n",
    "# ===================================================================================\n",
    "# 4. LightGBM Training with Hyperparameter Tuning\n",
    "# ===================================================================================\n",
    "def train_lgb(X, y, groups, X_test):\n",
    "    print(\"📈 Training LightGBM model...\")\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    oof = np.zeros(len(X))\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "\n",
    "    params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'map',\n",
    "        'eval_at': 7,\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 64,\n",
    "        'n_estimators': 300,\n",
    "        'verbose': -1\n",
    "    }\n",
    "\n",
    "    for fold, (t_idx, v_idx) in enumerate(gkf.split(X, y, groups)):\n",
    "        print(f\"\\n🔁 Fold {fold+1}/5\")\n",
    "        Xt, Xv = X.iloc[t_idx], X.iloc[v_idx]\n",
    "        yt, yv = y.iloc[t_idx], y.iloc[v_idx]\n",
    "\n",
    "        gr_t = Xt['id2'].value_counts().sort_index().values\n",
    "        gr_v = Xv['id2'].value_counts().sort_index().values\n",
    "\n",
    "        Xt_, Xv_ = Xt.drop('id2', axis=1), Xv.drop('id2', axis=1)\n",
    "        Xtest_ = X_test.drop('id2', axis=1)\n",
    "\n",
    "        model = lgb.LGBMRanker(**params)\n",
    "        model.fit(Xt_, yt, group=gr_t, eval_set=[(Xv_, yv)], eval_group=[gr_v], callbacks=[lgb.early_stopping(50)])\n",
    "        oof[v_idx] = model.predict(Xv_)\n",
    "        test_preds += model.predict(Xtest_)\n",
    "\n",
    "    test_preds /= gkf.get_n_splits()\n",
    "    return oof, test_preds\n",
    "\n",
    "# ===================================================================================\n",
    "# 5. Inference and Submission\n",
    "# ===================================================================================\n",
    "oof, test_preds = train_lgb(X, y, X['id2'], X_test)\n",
    "train['pred'] = oof\n",
    "\n",
    "print(\"📤 Generating predictions and evaluating MAP@7...\")\n",
    "oof_ranked = train.sort_values(['id2','pred'], ascending=[True,False]).groupby('id2')['id3'].apply(list).reset_index(name='predicted_offers')\n",
    "true_offers = train[train['y']==1].groupby('id2')['id3'].apply(list).reset_index(name='true_offers')\n",
    "eval_df = oof_ranked.merge(true_offers, on='id2', how='left')\n",
    "eval_df['true_offers'] = eval_df['true_offers'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "final_score = mapk(eval_df['true_offers'], eval_df['predicted_offers'])\n",
    "print(f\"\\n📊 Final OOF MAP@7 Score: {final_score:.6f}\")\n",
    "\n",
    "print(\"🧪 Generating submission file...\")\n",
    "scaler = MinMaxScaler()\n",
    "test['pred'] = scaler.fit_transform(test_preds.reshape(-1, 1)).flatten()\n",
    "submission = test[['id1', 'id2', 'id3', 'id5', 'pred']].copy()\n",
    "submission['id5'] = pd.to_datetime(submission['id5']).dt.strftime('%m/%d/%Y')\n",
    "submission.to_csv('submissionvery.csv', index=False)\n",
    "print(\"\\n✅ Submission file saved as 'submission.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ae09aa-3d0e-43c4-9428-fd710914747a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
